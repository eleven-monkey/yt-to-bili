# -*- coding: utf-8 -*-
import os
import sys
import re
import json
import time
import shutil
import subprocess
import asyncio
import glob
import random
import threading
import traceback
from datetime import datetime
from pathlib import Path
import tempfile
import zipfile
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from PIL import Image

import streamlit as st
import yt_dlp
import requests

def run_yt_dlp_subprocess(args, cookies_path=None):
    # Prefer calling yt-dlp directly to avoid python -m issues (like 'main.py error')
    cmd = [
        'yt-dlp',
        '--extractor-args', 'youtube:player_client=default,-web_safari',
        '--remote-components', 'ejs:github',
        '--no-playlist'
    ]
    if cookies_path:
        cmd.extend(['--cookies', cookies_path])
    
    cmd.extend(args)
    
    # Debug info
    print(f"Executing yt-dlp command: {' '.join(cmd)}")
    
    # Use shell=False for security, assuming args are clean
    result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore')
    
    if result.returncode != 0:
        # Fallback to python -m yt_dlp if binary is not found, but log it
        if "No such file or directory" in str(result.stderr) or result.returncode == 127:
             print("yt-dlp binary not found, falling back to python -m yt_dlp")
             import sys
             cmd[0] = sys.executable
             cmd.insert(1, '-m')
             cmd.insert(2, 'yt_dlp')
             print(f"Executing fallback command: {' '.join(cmd)}")
             result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore')

        if result.returncode != 0:
            raise Exception(f"yt-dlp error: {result.stderr}")
    return result.stdout

import edge_tts
from bilibili_api import sync, video_uploader, Credential
from bilibili_api.video_uploader import VideoUploaderPage, VideoMeta
import pydub
import pickle
from worker_utils import process_segment, adjust_audio_speed

def load_env_config():
    """
    åŠ è½½é…ç½®ï¼šä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡(HuggingFace Secrets)ï¼Œå…¶æ¬¡ä½¿ç”¨.envæ–‡ä»¶
    """
    config = {}
    
    # 1. å…ˆåŠ è½½ .env æ–‡ä»¶ (å¦‚æœæœ‰)
    env_file = os.path.join(os.getcwd(), ".env")
    if os.path.exists(env_file):
        try:
            with open(env_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        config[key.strip()] = value.strip()
        except Exception as e:
            print(f"è¯»å– .env æ–‡ä»¶å¤±è´¥: {e}")

    # 2. å†åŠ è½½ç³»ç»Ÿç¯å¢ƒå˜é‡ (è¦†ç›– .env ä¸­çš„åŒåé…ç½®)
    # æˆ‘ä»¬å…³å¿ƒçš„ç‰¹å®šç¯å¢ƒå˜é‡åˆ—è¡¨
    target_keys = [
        "API_KEY", "API_URL", "MODEL_NAME", 
        "YT_COOKIES", 
        "BILI_SESSDATA", "BILI_BILI_JCT", "BILI_BUVID3", 
        "BILI_ACCESS_KEY_ID", "BILI_ACCESS_KEY_SECRET"
    ]
    
    for key in target_keys:
        env_val = os.getenv(key)
        if env_val:
            if key == "YT_COOKIES":
                env_val = env_val.replace("\\n", "\n")
            config[key] = env_val
            
    # å…¼å®¹æ—§çš„/æ‹¼å†™é”™è¯¯çš„å˜é‡å BILI_SESSIDATA -> BILI_SESSDATA
    if "BILI_SESSDATA" not in config and os.getenv("BILI_SESSIDATA"):
        config["BILI_SESSDATA"] = os.getenv("BILI_SESSIDATA")
            
    return config

env_config = load_env_config()

# --- çŠ¶æ€ç®¡ç†ä¸åå°ä»»åŠ¡ç›¸å…³ ---
STATUS_FILE = "workflow_status.json"

class WorkflowManager:
    @staticmethod
    def get_status_file_path(temp_dir):
        return os.path.join(temp_dir, STATUS_FILE)

    @staticmethod
    def init_status(temp_dir):
        status = {
            "is_running": True,
            "stop_requested": False,
            "start_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "steps": {
                "ä¸‹è½½å­—å¹•": {"status": "pending", "message": ""},
                "ç¿»è¯‘æ ‡é¢˜": {"status": "pending", "message": ""},
                "ç¿»è¯‘å­—å¹•": {"status": "pending", "message": ""},
                "è½¬è¯­éŸ³": {"status": "pending", "message": ""},
                "ä¸‹è½½è§†é¢‘": {"status": "pending", "message": ""},
                "å¤„ç†å°é¢": {"status": "pending", "message": ""},
                "ä¸Šä¼ Bç«™": {"status": "pending", "message": ""}
            },
            "results": {},
            "error": None,
            "logs": []
        }
        WorkflowManager.save_status(temp_dir, status)
        return status

    @staticmethod
    def request_stop(temp_dir):
        current_status = WorkflowManager.load_status(temp_dir)
        if current_status:
            current_status["stop_requested"] = True
            current_status["is_running"] = False  # ç«‹å³æ ‡è®°ä¸ºåœæ­¢
            current_status["logs"].append(f"[{datetime.now().strftime('%H:%M:%S')}] ç”¨æˆ·è¯·æ±‚ä¸­æ­¢ä»»åŠ¡...")
            WorkflowManager.save_status(temp_dir, current_status)

    @staticmethod
    def load_status(temp_dir):
        file_path = WorkflowManager.get_status_file_path(temp_dir)
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return None
        return None

    @staticmethod
    def save_status(temp_dir, status):
        file_path = WorkflowManager.get_status_file_path(temp_dir)
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(status, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")

    @staticmethod
    def update_step(temp_dir, step_name, status_code, message=""):
        """
        status_code: pending, running, success, error
        """
        current_status = WorkflowManager.load_status(temp_dir)
        if current_status:
            if step_name in current_status["steps"]:
                current_status["steps"][step_name]["status"] = status_code
                current_status["steps"][step_name]["message"] = message
            current_status["logs"].append(f"[{datetime.now().strftime('%H:%M:%S')}] {step_name}: {status_code} - {message}")
            WorkflowManager.save_status(temp_dir, current_status)

    @staticmethod
    def mark_completed(temp_dir, results=None):
        current_status = WorkflowManager.load_status(temp_dir)
        if current_status:
            current_status["is_running"] = False
            if results:
                current_status["results"].update(results)
            WorkflowManager.save_status(temp_dir, current_status)

    @staticmethod
    def mark_error(temp_dir, error_msg):
        current_status = WorkflowManager.load_status(temp_dir)
        if current_status:
            current_status["is_running"] = False
            current_status["error"] = error_msg
            WorkflowManager.save_status(temp_dir, current_status)

def background_workflow_task(config):
    """
    åå°è¿è¡Œçš„å·¥ä½œæµä¸»å‡½æ•°
    config: åŒ…å«æ‰€æœ‰å¿…è¦å‚æ•°çš„å­—å…¸
    """
    temp_dir = config['temp_dir']
    workflow_url = config['workflow_url']
    auto_upload = config['auto_upload']
    
    # åˆå§‹åŒ–çŠ¶æ€
    WorkflowManager.init_status(temp_dir)
    
    def check_interrupt():
        """æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­è¯·æ±‚"""
        s = WorkflowManager.load_status(temp_dir)
        if s and s.get("stop_requested", False):
            raise Exception("ç”¨æˆ·æ‰‹åŠ¨ä¸­æ­¢ä»»åŠ¡")

    try:
        check_interrupt()
        subtitles_dir = os.path.join(temp_dir, "subtitles")
        os.makedirs(subtitles_dir, exist_ok=True)
        
        # --- æ­¥éª¤1: ä¸‹è½½å­—å¹• ---
        check_interrupt()
        WorkflowManager.update_step(temp_dir, "ä¸‹è½½å­—å¹•", "running", "æ­£åœ¨ä¸‹è½½å­—å¹•...")
        
        cookies_file_path = None
        if config.get('yt_cookies', '').strip():
            cookies_file_path = os.path.join(temp_dir, "youtube_cookies.txt")
            with open(cookies_file_path, 'w', encoding='utf-8') as f:
                f.write(config['yt_cookies'].strip())
        
        # é‡è¯•æœºåˆ¶
        def retry_op(func, max_retries=3):
            for attempt in range(max_retries):
                try:
                    check_interrupt()
                    return func()
                except Exception as e:
                    if str(e) == "ç”¨æˆ·æ‰‹åŠ¨ä¸­æ­¢ä»»åŠ¡": raise e
                    if attempt == max_retries - 1: raise e
                    # é‡è¯•å‰æ£€æŸ¥ä¸­æ–­
                    check_interrupt()
                    time.sleep(2 ** attempt)

        def dl_sub():
            args = [
                '--write-auto-sub',
                '--skip-download',
                '--sub-langs', 'en',
                '--quiet',
                '-o', os.path.join(subtitles_dir, '%(title)s.%(ext)s'),
                workflow_url
            ]
            run_yt_dlp_subprocess(args, cookies_file_path)
            vtt_files = list(Path(subtitles_dir).glob("*.vtt"))
            if not vtt_files: raise Exception("æœªæ‰¾åˆ°VTTæ–‡ä»¶")
            original_file = vtt_files[0]
            new_file = os.path.join(subtitles_dir, "word_level.vtt")
            # å¦‚æœå­˜åœ¨åˆ™è¦†ç›–
            if os.path.exists(new_file): os.remove(new_file)
            os.rename(original_file, new_file)
            return new_file

        vtt_file_path = retry_op(dl_sub)
        WorkflowManager.update_step(temp_dir, "ä¸‹è½½å­—å¹•", "success", f"å·²ä¿å­˜: {os.path.basename(vtt_file_path)}")
        
        # --- æ­¥éª¤2: ç¿»è¯‘æ ‡é¢˜ ---
        check_interrupt()
        WorkflowManager.update_step(temp_dir, "ç¿»è¯‘æ ‡é¢˜", "running", "æ­£åœ¨åˆ†æè§†é¢‘ä¿¡æ¯...")
        
        def trans_title():
            args = ['--dump-json', '--skip-download', '--quiet', workflow_url]
            stdout = run_yt_dlp_subprocess(args, cookies_file_path)
            info_dict = json.loads(stdout)
            original_title = info_dict.get('title', '')
            
            if not original_title: raise Exception("æ— æ³•è·å–æ ‡é¢˜")
            
            # è°ƒç”¨APIç¿»è¯‘
            headers = {"Authorization": f"Bearer {config['api_key']}", "Content-Type": "application/json"}
            
            # æ ‡é¢˜ç¿»è¯‘
            payload = {
                "model": config['model_name'],
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯çˆ†æ¬¾è§†é¢‘upä¸»ï¼Œå°†è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆå¸å¼•çœ¼çƒçš„çˆ†æ¬¾è§†é¢‘ä¸­æ–‡æ ‡é¢˜ï¼Œç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦è§£é‡Šã€‚"},
                    {"role": "user", "content": original_title}
                ]
            }
            resp = requests.post(config['api_url'], json=payload, headers=headers, timeout=60)
            translated_title = resp.json()['choices'][0]['message']['content'].replace('**', '').strip()
            
            # æ ‡ç­¾ç”Ÿæˆ
            tags_payload = {
                "model": config['model_name'],
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Bç«™è¿è¥åŠ©æ‰‹"},
                    {"role": "user", "content": f"æ ¹æ®ä»¥ä¸‹è§†é¢‘æ ‡é¢˜ï¼Œç”Ÿæˆ5-8ä¸ªBç«™è§†é¢‘æ ‡ç­¾ï¼ˆåªè¾“å‡ºæ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”ï¼‰ï¼š\næ ‡é¢˜ï¼š{translated_title}\nåªè¾“å‡ºæ ‡ç­¾ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"}
                ]
            }
            tags_resp = requests.post(config['api_url'], json=tags_payload, headers=headers, timeout=60)
            tags_str = tags_resp.json()['choices'][0]['message']['content']
            tags_list = [t.strip() for t in tags_str.replace('ï¼Œ', ',').split(',') if t.strip()][:10]
            
            # ä¿å­˜ä¸Šä¼ é…ç½®
            upload_data = {'title_desc': f'(ä¸­é…){translated_title}', 'tags': tags_list}
            with open(os.path.join(subtitles_dir, "upload_config.pkl"), 'wb') as f:
                pickle.dump(upload_data, f)
                
            return translated_title, tags_list
            
        translated_title, tags_list = retry_op(trans_title)
        WorkflowManager.update_step(temp_dir, "ç¿»è¯‘æ ‡é¢˜", "success", f"æ ‡é¢˜: {translated_title}")
        
        # --- æ­¥éª¤3: ç¿»è¯‘å­—å¹• ---
        check_interrupt()
        WorkflowManager.update_step(temp_dir, "ç¿»è¯‘å­—å¹•", "running", "AIæ­£åœ¨ç¿»è¯‘ä¸­(å¯èƒ½è¾ƒæ…¢)...")
        # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨å…¨å±€å‡½æ•°ï¼Œå®ƒä¼šæ‰“å°æ—¥å¿—åˆ°stdoutï¼Œä½†æˆ‘ä»¬éœ€è¦å®ƒæ­£å¸¸è¿è¡Œ
        # æˆ‘ä»¬å¯ä»¥æš‚æ—¶ä¸æ•è·å®ƒçš„è¯¦ç»†è¿›åº¦ï¼Œæˆ–è€…ä¿®æ”¹åŸå‡½æ•°ã€‚ä¸ºä¿æŒæœ€å°æ”¹åŠ¨ï¼Œç›´æ¥è°ƒç”¨ã€‚
        # è¿™é‡Œçš„å…¨å±€å˜é‡ API_URL, API_KEY ç­‰éœ€è¦åœ¨è°ƒç”¨å‰ä¸´æ—¶è¦†ç›–å—ï¼Ÿ
        # å…¨å±€å‡½æ•° translate_subtitles_from_vtt ä½¿ç”¨äº†å…¨å±€å˜é‡ API_URL ç­‰ã€‚
        # åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ä¿®æ”¹å…¨å±€å˜é‡æ˜¯å±é™©çš„ã€‚
        # ä½†è¿™é‡Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ python çš„åŠ¨æ€æ€§ï¼Œæˆ–è€…å‡è®¾ config ä¸­çš„å€¼å’Œå…¨å±€çš„ä¸€è‡´ã€‚
        # å®é™…ä¸Š app.py é‡Œçš„ API_URL æ˜¯ä» st.sidebar è·å–çš„ï¼Œåœ¨åå°çº¿ç¨‹é‡Œæ— æ³•è®¿é—® st.sidebarã€‚
        # å¿…é¡»ç¡®ä¿å…¨å±€å˜é‡è¢«æ­£ç¡®è®¾ç½®ï¼Œæˆ–è€…é‡æ„ translate_subtitles_from_vtt æ¥å—å‚æ•°ã€‚
        # ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬è¿™é‡Œåšä¸€ä¸ªç®€å•çš„ trickï¼šåœ¨ app.py é¡¶å±‚ï¼ŒAPI_URL ç­‰æ˜¯å…¨å±€å˜é‡ã€‚
        # ç”¨æˆ·åœ¨ UI ä¿®æ”¹åï¼Œè¿™äº›å…¨å±€å˜é‡å¹¶æ²¡æœ‰å˜ï¼ˆå®ƒä»¬åªæ˜¯è„šæœ¬é¡¶å±‚çš„åˆå§‹å€¼ï¼‰ã€‚
        # st.sidebar çš„å€¼æ˜¯åœ¨ st è¿è¡Œæ—¶è·å–çš„ã€‚
        # è¿™æ˜¯ä¸€ä¸ªæ½œåœ¨ BUGï¼šåŸä»£ç ä¸­ translate_subtitles_from_vtt ç›´æ¥ç”¨äº† API_URLã€‚
        # åœ¨ Streamlit ä¸­ï¼Œæ¯æ¬¡ rerun æ•´ä¸ªè„šæœ¬ä»å¤´è·‘ï¼Œå…¨å±€å˜é‡é‡ç½®ã€‚
        # å½“ç‚¹å‡»æŒ‰é’®æ—¶ï¼ŒAPI_URL æ˜¯å½“å‰å±€éƒ¨å˜é‡ï¼ˆå¦‚æœæ˜¯é€šè¿‡ st.sidebar... è¿”å›çš„ï¼‰ã€‚
        # åŸä»£ç ä¸­ï¼šAPI_URL = st.sidebar.text_input(...)
        # æ‰€ä»¥ API_URL åœ¨è„šæœ¬æ‰§è¡ŒåŸŸä¸­æ˜¯å­˜åœ¨çš„ã€‚
        # ä½†æ˜¯ï¼Œå½“çº¿ç¨‹è¿è¡Œæ—¶ï¼Œå¦‚æœä¸»çº¿ç¨‹ï¼ˆStreamlit runnerï¼‰ç»“æŸæˆ– rerunï¼Œè¿™äº›æ¨¡å—çº§å˜é‡è¿˜åœ¨å—ï¼Ÿ
        # åœ¨ Streamlit ä¸­ï¼Œæ¨¡å—çº§åˆ«çš„å˜é‡æ˜¯è·¨ session å…±äº«çš„ï¼ˆå¦‚æœä¸æ˜¯åœ¨å‡½æ•°å†…å®šä¹‰ï¼‰ã€‚
        # ä½† API_URL = st.sidebar... æ˜¯åœ¨è„šæœ¬æ‰§è¡Œæµä¸­å®šä¹‰çš„ã€‚
        # ä¸ºäº†ç¡®ä¿åå°çº¿ç¨‹èƒ½æ‹¿åˆ°æ­£ç¡®çš„é…ç½®ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹ translate_subtitles_from_vtt 
        # æˆ–è€…ä¸´æ—¶è®¾ç½®å…¨å±€å˜é‡ï¼ˆä¸æ¨èï¼‰ã€‚
        # æœ€ç¨³å¦¥çš„æ–¹æ³•ï¼šä¿®æ”¹ translate_subtitles_from_vtt ç­¾åæ¥å— api_key ç­‰å‚æ•°ã€‚
        # ä½†ç”¨æˆ·è¦æ±‚ "ä¸è¦ç€æ€¥ç¼–ç " ä¸” "mimic style"ï¼Œæˆ‘é€‰æ‹©ä¸€ç§ä¾µå…¥æ€§å°çš„æ–¹æ³•ï¼š
        # å°†é…ç½®æ³¨å…¥åˆ°å…¨å±€ï¼ˆè™½ç„¶æœ‰ç‚¹è„ï¼Œä½†åœ¨å•å®ä¾‹å®¹å™¨ä¸­å¯è¡Œï¼‰ï¼Œæˆ–è€…æœ€å¥½ç¨å¾®ä¿®æ”¹ä¸€ä¸‹ translate_subtitles_from_vttã€‚
        # è®©æˆ‘ä»¬çœ‹çœ‹ translate_subtitles_from_vtt å®šä¹‰ã€‚å®ƒç¡®å®ä½¿ç”¨äº†å…¨å±€ API_URLã€‚
        # æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ unittest.mock.patch æˆ–è€…ç®€å•çš„ global èµ‹å€¼æ¥ç¡®ä¿çº¿ç¨‹å†…çœ‹åˆ°çš„å˜é‡æ˜¯å¯¹çš„ã€‚
        # ä½†ç”±äºè¿™æ˜¯å¤šçº¿ç¨‹ï¼Œä¿®æ”¹å…¨å±€å˜é‡ä¼šå½±å“å…¶ä»–ç”¨æˆ·ï¼ˆè™½ç„¶ streamlit é€šå¸¸å•å®ä¾‹ï¼‰ã€‚
        # æ›´å¥½çš„æ–¹æ¡ˆï¼šä¼ é€’å‚æ•°ã€‚
        # æˆ‘å°†ä¿®æ”¹ translate_subtitles_from_vtt åŠå…¶è°ƒç”¨çš„å‡½æ•°ï¼Œä½†è¿™æ”¹åŠ¨å¤§ã€‚
        # è®©æˆ‘ä»¬ç”¨ global å˜é‡æ³¨å…¥çš„æ–¹å¼ï¼ˆåœ¨çº¿ç¨‹å¼€å§‹å‰ï¼Œæˆ–è€…å‡è®¾ç”¨æˆ·æ²¡æœ‰å˜ï¼‰ã€‚
        # å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥é‡å†™ translate_subtitles_from_vtt çš„éƒ¨åˆ†é€»è¾‘åœ¨çº¿ç¨‹é‡Œï¼Œæˆ–è€…ï¼Œ
        # é‰´äº `translate_subtitles_from_vtt` å°±åœ¨ `app.py` é‡Œï¼Œ
        # æˆ‘å¯ä»¥ç®€å•åœ°å°†è¿™äº›é…ç½®ä½œä¸ºå‚æ•°ä¼ é€’ç»™ `translate_subtitles_from_vtt`ï¼Œç»™å®ƒåŠ é»˜è®¤å‚æ•°=Noneï¼Œå¦‚æœNoneåˆ™å–å…¨å±€ã€‚
        # è¿™æ ·æ”¹åŠ¨æœ€å°ã€‚
        
        # ç¨åæˆ‘ä¼šå¾®è°ƒ translate_subtitles_from_vttã€‚ç°åœ¨å…ˆå‡è®¾å®ƒèƒ½å·¥ä½œï¼ˆå¦‚æœå®ƒå¼•ç”¨çš„å…¨å±€å˜é‡è¢«æ­£ç¡®é—­åŒ…æ•è·ï¼‰ã€‚
        # å®é™…ä¸Š python çš„é—­åŒ…æ˜¯è¿Ÿç»‘å®šçš„ã€‚
        # ä¸ºäº†ç¨³å¦¥ï¼Œæˆ‘åœ¨ background_workflow_task é‡Œå®šä¹‰ä¸€ä¸ª wrapper æˆ–è€… monkeypatchã€‚
        # è®©æˆ‘ä»¬å°è¯•ä¸€ç§ Pythonic çš„æ–¹æ³•ï¼šåŠ¨æ€ä¿®æ”¹å…¨å±€å˜é‡ä¸Šä¸‹æ–‡ï¼Ÿä¸ï¼Œå¤ªé»‘é­”æ³•ã€‚
        # æˆ‘å°†ä¿®æ”¹ `translate_subtitles_from_vtt` æ¥å—å¯é€‰çš„ api_config å‚æ•°ã€‚
        
        txt_file_path = translate_subtitles_from_vtt(vtt_file_path, api_config={
            "API_URL": config['api_url'],
            "API_KEY": config['api_key'],
            "MODEL_NAME": config['model_name'],
            "MAX_WORKERS": config['max_workers'],
            "SEGMENT_SIZE": config['segment_size']
        })
        
        WorkflowManager.update_step(temp_dir, "ç¿»è¯‘å­—å¹•", "success", f"å·²ä¿å­˜: {os.path.basename(txt_file_path)}")
        
        # --- æ­¥éª¤4: è½¬è¯­éŸ³ ---
        check_interrupt()
        WorkflowManager.update_step(temp_dir, "è½¬è¯­éŸ³", "running", "æ­£åœ¨è¿›è¡ŒTTSè½¬æ¢...")
        
        output_mp3 = os.path.join(subtitles_dir, os.path.splitext(os.path.basename(vtt_file_path))[0] + "_translated.mp3")
        # åŒç†ï¼Œprocess_tts_with_speed_adjustment ä¹Ÿéœ€è¦ config
        mp3_file_path = process_tts_with_speed_adjustment(txt_file_path, output_mp3, subtitles_dir, tts_config={
            "TEMP_DIR": temp_dir,
            "SELECTED_VOICE": config['voice_choice']
        })
        
        if not mp3_file_path: raise Exception("TTSè½¬æ¢å¤±è´¥")
        WorkflowManager.update_step(temp_dir, "è½¬è¯­éŸ³", "success", f"å·²ç”Ÿæˆ: {os.path.basename(mp3_file_path)}")
        
        # --- æ­¥éª¤5: ä¸‹è½½è§†é¢‘ ---
        check_interrupt()
        WorkflowManager.update_step(temp_dir, "ä¸‹è½½è§†é¢‘", "running", "ä¸‹è½½å¹¶åˆå¹¶è§†é¢‘...")
        
        def dl_video():
            dl_base = os.path.join(temp_dir, "subtitles", "downloaded_video")
            args = [
                '-f', 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',
                '-o', f'{dl_base}.%(ext)s',
                '--quiet',
                workflow_url
            ]
            run_yt_dlp_subprocess(args, cookies_file_path)
            
            dl_files = glob.glob(f"{dl_base}.*")
            if not dl_files: raise Exception("è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°")
            
            raw_video = dl_files[0]
            final_vid = os.path.splitext(mp3_file_path)[0] + ".mp4"
            
            subprocess.run(['ffmpeg', '-y', '-i', raw_video, '-i', mp3_file_path,
                           '-c:v', 'copy', '-c:a', 'aac', '-map', '0:v:0', '-map', '1:a:0',
                           final_vid], check=True, capture_output=True)
            
            if os.path.exists(raw_video): os.remove(raw_video)
            return final_vid
            
        final_video_path = retry_op(dl_video)
        WorkflowManager.update_step(temp_dir, "ä¸‹è½½è§†é¢‘", "success", f"æœ€ç»ˆè§†é¢‘: {os.path.basename(final_video_path)}")
        
        # --- æ­¥éª¤6: å¤„ç†å°é¢ ---
        check_interrupt()
        WorkflowManager.update_step(temp_dir, "å¤„ç†å°é¢", "running", "ä¼˜åŒ–å°é¢å›¾ç‰‡...")
        
        def proc_cover():
            args = [
                '--skip-download',
                '--write-thumbnail',
                '--quiet',
                '-o', os.path.join(temp_dir, "subtitles", 'cover.%(ext)s'),
                workflow_url
            ]
            run_yt_dlp_subprocess(args, cookies_file_path)
            
            # å¯»æ‰¾å°é¢æ–‡ä»¶
            cover_candidates = list(Path(os.path.join(temp_dir, "subtitles")).glob("cover.*"))
            # æ’é™¤å·²ç»æ˜¯jpegçš„é˜²æ­¢é‡å¤å¤„ç†
            src_cover = None
            for c in cover_candidates:
                if c.suffix.lower() in ['.webp', '.jpg', '.png']:
                    src_cover = c
                    break
            
            if not src_cover: raise Exception("æœªæ‰¾åˆ°å°é¢æ–‡ä»¶")
            
            out_cover = os.path.join(temp_dir, "subtitles", "cover.jpeg")
            qual = 90
            with Image.open(src_cover) as img:
                if img.mode != 'RGB': img = img.convert('RGB')
                img.save(out_cover, 'jpeg', quality=qual)
                
                # å‹ç¼©åˆ°åˆé€‚å¤§å°
                while os.path.getsize(out_cover) / 1024 > 50 and qual > 10:
                    qual -= 5
                    img.save(out_cover, 'jpeg', quality=qual)
            return out_cover

        cover_path = retry_op(proc_cover)
        WorkflowManager.update_step(temp_dir, "å¤„ç†å°é¢", "success", "å°é¢å¤„ç†å®Œæˆ")
        
        results = {
            "vtt": vtt_file_path,
            "txt": txt_file_path,
            "mp3": mp3_file_path,
            "video": final_video_path,
            "cover": cover_path
        }
        
        # --- æ­¥éª¤7: ä¸Šä¼ Bç«™ ---
        if auto_upload:
            check_interrupt()
            WorkflowManager.update_step(temp_dir, "ä¸Šä¼ Bç«™", "running", "æ­£åœ¨ä¸Šä¼ åˆ°Bç«™...")
            
            print("=" * 50, flush=True)
            print("å¼€å§‹Bç«™ä¸Šä¼ æµç¨‹", flush=True)
            print("=" * 50, flush=True)
            
            credential = Credential(sessdata=config['bili_sess'], bili_jct="bcd4ba0d9ab8a7b95485798ed8097d26")
            vu_meta = VideoMeta(
                tid=130, title=translated_title, tags=tags_list,
                desc=translated_title, cover=cover_path, no_reprint=True
            )
            
            upload_completed = False
            upload_error = None
            upload_result = None
            
            async def upload_task():
                nonlocal upload_completed, upload_error, upload_result
                try:
                    page = VideoUploaderPage(path=final_video_path, title=translated_title, description=translated_title)
                    uploader = video_uploader.VideoUploader([page], vu_meta, credential, line=video_uploader.Lines.QN)
                    
                    # è¿›åº¦çŠ¶æ€
                    total_chunks = 0
                    uploaded_chunks = 0
                    last_percent = 0
                    
                    @uploader.on("__ALL__")
                    async def on_all_event(event_data):
                        nonlocal total_chunks, uploaded_chunks, last_percent
                        
                        # æ£€æŸ¥ä¸­æ–­
                        check_interrupt()
                        
                        # å¤„ç† tuple ç±»å‹çš„ event_data
                        if isinstance(event_data, tuple):
                            if len(event_data) > 0:
                                event_data = event_data[0]
                            else:
                                event_data = {}
                                
                        # æ‰“å°äº‹ä»¶åç§°å’Œå…³é”®æ•°æ®
                        event_name = event_data.get("name", "UNKNOWN")
                        data = event_data.get("data", {})
                        
                        if event_name == "PREUPLOAD":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ PREUPLOAD - è·å–ä¸Šä¼ ä¿¡æ¯ä¸­...", flush=True)
                        elif event_name == "PREUPLOAD_FAILED":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ PREUPLOAD_FAILED - è·å–ä¸Šä¼ ä¿¡æ¯å¤±è´¥: {data}", flush=True)
                        elif event_name == "PRE_CHUNK":
                            total_chunks = data.get("total_chunk_count", 1)
                            chunk_num = data.get("chunk_number", 1)
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ PRE_CHUNK - å¼€å§‹ä¸Šä¼ åˆ†å— {chunk_num}/{total_chunks}", flush=True)
                        elif event_name == "AFTER_CHUNK":
                            uploaded_chunks += 1
                            chunk_num = data.get("chunk_number", 0)
                            percent = int((uploaded_chunks / total_chunks) * 100) if total_chunks > 0 else 0
                            if percent - last_percent >= 5:
                                print(f"[ä¸Šä¼ ] äº‹ä»¶ AFTER_CHUNK - åˆ†å— {chunk_num} ä¸Šä¼ å®Œæˆ, è¿›åº¦ {percent}%", flush=True)
                                WorkflowManager.update_step(temp_dir, "ä¸Šä¼ Bç«™", "running", f"ä¸Šä¼ ä¸­... {percent}%")
                                last_percent = percent
                        elif event_name == "CHUNK_FAILED":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ CHUNK_FAILED - åˆ†å—ä¸Šä¼ å¤±è´¥: {data}", flush=True)
                        elif event_name == "PRE_PAGE_SUBMIT":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ PRE_PAGE_SUBMIT - å‡†å¤‡æäº¤åˆ†P", flush=True)
                        elif event_name == "AFTER_PAGE_SUBMIT":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ AFTER_PAGE_SUBMIT - åˆ†Pæäº¤å®Œæˆ", flush=True)
                        elif event_name == "PAGE_SUBMIT_FAILED":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ PAGE_SUBMIT_FAILED - åˆ†Pæäº¤å¤±è´¥: {data}", flush=True)
                        elif event_name == "PRE_COVER":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ PRE_COVER - å‡†å¤‡ä¸Šä¼ å°é¢", flush=True)
                        elif event_name == "AFTER_COVER":
                            cover_url = data.get("url", "")
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ AFTER_COVER - å°é¢ä¸Šä¼ å®Œæˆ: {cover_url}", flush=True)
                        elif event_name == "COVER_FAILED":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ COVER_FAILED - å°é¢ä¸Šä¼ å¤±è´¥: {data}", flush=True)
                        elif event_name == "PRE_SUBMIT":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ PRE_SUBMIT - å‡†å¤‡æœ€ç»ˆæäº¤", flush=True)
                        elif event_name == "SUBMIT_FAILED":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ SUBMIT_FAILED - æœ€ç»ˆæäº¤å¤±è´¥: {data}", flush=True)
                        elif event_name == "AFTER_SUBMIT":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ AFTER_SUBMIT - æœ€ç»ˆæäº¤å®Œæˆ", flush=True)
                            upload_result = data
                        elif event_name == "COMPLETED":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ COMPLETED - ä¸Šä¼ å…¨éƒ¨å®Œæˆ", flush=True)
                        elif event_name == "ABORTED":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ ABORTED - ä¸Šä¼ è¢«ä¸­æ­¢", flush=True)
                        elif event_name == "FAILED":
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ FAILED - ä¸Šä¼ å¤±è´¥: {data}", flush=True)
                        else:
                            print(f"[ä¸Šä¼ ] äº‹ä»¶ {event_name}: {data}", flush=True)
                    
                    print(f"[ä¸Šä¼ ] å¼€å§‹è°ƒç”¨ uploader.start()...", flush=True)
                    
                    # è®¾ç½®è¶…æ—¶ï¼ˆ10åˆ†é’Ÿï¼‰
                    await asyncio.wait_for(uploader.start(), timeout=600)
                    upload_completed = True
                    print(f"[ä¸Šä¼ ] uploader.start() è¿”å›ï¼Œä¸Šä¼ å®Œæˆ", flush=True)
                    
                except asyncio.TimeoutError:
                    upload_error = "ä¸Šä¼ è¶…æ—¶ï¼ˆ10åˆ†é’Ÿï¼‰ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–Bç«™æœåŠ¡å™¨ç¹å¿™"
                    print(f"[ä¸Šä¼ ] è¶…æ—¶é”™è¯¯: {upload_error}", flush=True)
                except asyncio.CancelledError:
                    upload_error = "ä¸Šä¼ è¢«ç”¨æˆ·å–æ¶ˆ"
                    print(f"[ä¸Šä¼ ] å–æ¶ˆé”™è¯¯: {upload_error}", flush=True)
                except Exception as e:
                    upload_error = str(e)
                    print(f"[ä¸Šä¼ ] å¼‚å¸¸é”™è¯¯: {upload_error}", flush=True)
                    import traceback
                    traceback.print_exc()
            
            # åœ¨æ–°äº‹ä»¶å¾ªç¯è¿è¡Œ
            print(f"[ä¸Šä¼ ] åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯", flush=True)
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(upload_task())
                print(f"[ä¸Šä¼ ] äº‹ä»¶å¾ªç¯æ‰§è¡Œå®Œæ¯•", flush=True)
            except Exception as e:
                print(f"[ä¸Šä¼ ] äº‹ä»¶å¾ªç¯å¼‚å¸¸: {e}", flush=True)
                import traceback
                traceback.print_exc()
            finally:
                loop.close()
                print(f"[ä¸Šä¼ ] äº‹ä»¶å¾ªç¯å·²å…³é—­", flush=True)
            
            # æ£€æŸ¥ä¸Šä¼ ç»“æœ
            if upload_error:
                WorkflowManager.update_step(temp_dir, "ä¸Šä¼ Bç«™", "error", upload_error)
                raise Exception(upload_error)
            elif not upload_completed:
                WorkflowManager.update_step(temp_dir, "ä¸Šä¼ Bç«™", "error", "ä¸Šä¼ æ„å¤–ä¸­æ–­ï¼ˆæœªå®Œæˆï¼‰")
                raise Exception("ä¸Šä¼ æ„å¤–ä¸­æ–­ï¼ˆæœªå®Œæˆï¼‰")
            
            if upload_result:
                bvid = upload_result.get("bvid", "")
                aid = upload_result.get("aid", "")
                print(f"[ä¸Šä¼ ] ä¸Šä¼ æˆåŠŸ! bvid={bvid}, aid={aid}", flush=True)
            
            WorkflowManager.update_step(temp_dir, "ä¸Šä¼ Bç«™", "success", "ä¸Šä¼ æˆåŠŸï¼")
            print(f"[ä¸Šä¼ ] Bç«™ä¸Šä¼ æ­¥éª¤å®Œæˆ", flush=True)
        else:
            WorkflowManager.update_step(temp_dir, "ä¸Šä¼ Bç«™", "success", "è·³è¿‡ä¸Šä¼ ")

        WorkflowManager.mark_completed(temp_dir, results)

    except Exception as e:
        import traceback
        err_msg = f"{str(e)}"
        if str(e) != "ç”¨æˆ·æ‰‹åŠ¨ä¸­æ­¢ä»»åŠ¡":
            err_msg += f"\n{traceback.format_exc()}"
            print(f"åå°ä»»åŠ¡å‡ºé”™: {err_msg}")
        WorkflowManager.mark_error(temp_dir, str(e))

def clear_temp_directory():
    """æ¸…ç©ºtempç›®å½•ä¸‹çš„æ‰€æœ‰å†…å®¹"""
    import shutil
    try:
        if os.path.exists(TEMP_DIR):
            for filename in os.listdir(TEMP_DIR):
                file_path = os.path.join(TEMP_DIR, filename)
                try:
                    if os.path.isfile(file_path) or os.path.islink(file_path):
                        os.unlink(file_path)
                    elif os.path.isdir(file_path):
                        shutil.rmtree(file_path)
                except Exception as e:
                    print(f'æ¸…ç©ºtempç›®å½•æ—¶å‡ºé”™ {file_path}: {e}')
            print("tempç›®å½•å·²æ¸…ç©º")
        else:
            os.makedirs(TEMP_DIR, exist_ok=True)
            print("tempç›®å½•å·²åˆ›å»º")
    except Exception as e:
        print(f"æ¸…ç©ºtempç›®å½•å¤±è´¥: {e}")

# ç¿»è¯‘å­—å¹•ç›¸å…³å‡½æ•°
def translate_subtitles_from_vtt(vtt_file_path, api_config=None):
    """ä»VTTæ–‡ä»¶ç¿»è¯‘å­—å¹•ï¼Œç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆå•æ­¥æ‰§è¡Œçš„å®Œæ•´é€»è¾‘ï¼‰"""
    # è·å–é…ç½®ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨å…¨å±€å˜é‡
    cfg_api_url = api_config.get("API_URL", API_URL) if api_config else API_URL
    cfg_api_key = api_config.get("API_KEY", API_KEY) if api_config else API_KEY
    cfg_model = api_config.get("MODEL_NAME", MODEL_NAME) if api_config else MODEL_NAME
    cfg_max_workers = api_config.get("MAX_WORKERS", MAX_WORKERS) if api_config else MAX_WORKERS
    cfg_seg_size = api_config.get("SEGMENT_SIZE", SEGMENT_SIZE) if api_config else SEGMENT_SIZE

    def vtt_to_sentences(vtt_text):
        """å°†å¸¦é€è¯æ—¶é—´æˆ³çš„VTTè½¬æ¢ä¸ºæŒ‰å¥åˆ†æ®µçš„æ–‡æœ¬"""
        # æ­£åˆ™ï¼šcue å¤´ï¼ˆèµ·æ­¢æ—¶é—´ï¼‰
        CUE_HEADER_RE = re.compile(
            r'^(\d{2}:\d{2}:\d{2}\.\d{3})\s*--> (\d{2}:\d{2}:\d{2}\.\d{3})'
        )

        # æ­£åˆ™ï¼šé€è¯æ—¶é—´æˆ³ <HH:MM:SS.mmm>
        TS_TAG_RE = re.compile(r'<(\d{2}:\d{2}:\d{2}\.\d{3})>')

        # æ­£åˆ™ï¼šæ¸…ç† <c> æˆ– <c.xxx> æ ·å¼æ ‡ç­¾
        C_TAG_RE = re.compile(r'</?c(?:\.[^>]*)?>', re.IGNORECASE)

        SENTENCE_END = ".!?"

        lines = vtt_text.splitlines()
        sentences = []
        current_words = []
        current_sentence_start_time = None

        effective_time = None
        cue_start_time = None

        def flush_sentence():
            nonlocal current_words, current_sentence_start_time
            if not current_words:
                return
            text = " ".join(current_words)
            text = re.sub(r"\s+([,.;!?])", r"\1", text)
            text = re.sub(r"\(\s+", "(", text)
            text = re.sub(r"\s+\)", ")", text)
            start_ts = current_sentence_start_time or cue_start_time or effective_time or "00:00:00.000"
            sentences.append(f"({start_ts}) {text}")
            current_words = []
            current_sentence_start_time = None

        for line in lines:
            line = line.strip("\ufeff\r\n")

            # cue å¤´
            m = CUE_HEADER_RE.match(line)
            if m:
                cue_start_time = m.group(1)
                effective_time = cue_start_time
                continue

            # åªå¤„ç†å«é€è¯æ—¶é—´æˆ³çš„è¡Œ
            if not TS_TAG_RE.search(line):
                continue

            # æ¸…ç† <c> æ ‡ç­¾ï¼Œå¹¶æŠŠ <timestamp> å˜æˆ [[TS:...]] å“¨å…µ
            s = C_TAG_RE.sub("", line)
            s = TS_TAG_RE.sub(lambda mm: f" [[TS:{mm.group(1)}]] ", s)

            # æ‰«æ token
            for token in s.split():
                if token.startswith("[[TS:") and token.endswith("]]"):
                    effective_time = token[5:-2]
                    continue

                word = token.strip()
                if not word:
                    continue

                # è®°å½•é¦–è¯æ—¶é—´
                if current_sentence_start_time is None:
                    current_sentence_start_time = effective_time or cue_start_time

                current_words.append(word)

                # å¥å­ç»“æŸåˆ¤å®šï¼ˆå¥å·ã€é—®å·ã€å¹å·ï¼‰
                if word.strip().endswith(tuple(SENTENCE_END)):
                    flush_sentence()

        # æ–‡ä»¶ç»“æŸï¼Œæ”¶å°¾
        flush_sentence()
        return sentences

    vtt_content = Path(vtt_file_path).read_text(encoding="utf-8", errors="ignore")
    sentences = vtt_to_sentences(vtt_content)

    print(f"è°ƒè¯•ä¿¡æ¯ï¼šè§£æå‡º {len(sentences)} ä¸ªå¥å­")
    if sentences:
        print(f"å‰3ä¸ªå¥å­ç¤ºä¾‹ï¼š")
        for i, s in enumerate(sentences[:3]):
            print(f"  {i+1}: {s[:100]}...")

    output_txt_file = os.path.splitext(vtt_file_path)[0] + ".txt"
    with open(output_txt_file, 'w', encoding='utf-8') as f:
        for seg in sentences:
            f.write(seg + "\n\n")

    paragraphs = [line.strip() for line in open(output_txt_file, 'r', encoding='utf-8') if line.strip()]

    print(f"è°ƒè¯•ä¿¡æ¯ï¼šè¯»å–åˆ° {len(paragraphs)} ä¸ªæ®µè½")

    batched_paragraphs = []
    current_batch = []
    current_char_count = 0

    for i, paragraph in enumerate(paragraphs):
        paragraph_char_count = len(paragraph)
        if (len(current_batch) >= cfg_seg_size) or (current_char_count + paragraph_char_count > 2000 and current_batch):
            batched_paragraphs.append("\n".join(current_batch))
            print(f"è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æ®µ {len(batched_paragraphs)} åŒ…å« {len(current_batch)} ä¸ªæ®µè½ï¼Œå…± {current_char_count} å­—ç¬¦")
            current_batch = [paragraph]
            current_char_count = paragraph_char_count
        else:
            current_batch.append(paragraph)
            current_char_count += paragraph_char_count

    if current_batch:
        batched_paragraphs.append("\n".join(current_batch))
        print(f"è°ƒè¯•ä¿¡æ¯ï¼šæœ€åä¸€ä¸ªåˆ†æ®µ {len(batched_paragraphs)} åŒ…å« {len(current_batch)} ä¸ªæ®µè½ï¼Œå…± {current_char_count} å­—ç¬¦")

    print(f"è°ƒè¯•ä¿¡æ¯ï¼šæ€»å…± {len(batched_paragraphs)} ä¸ªç¿»è¯‘åˆ†æ®µ")

    def translate_batch(batch, batch_index):
        try:
            print(f"è°ƒè¯•ä¿¡æ¯ï¼šå¼€å§‹ç¿»è¯‘åˆ†æ®µ {batch_index}ï¼Œå†…å®¹é•¿åº¦: {len(batch)} å­—ç¬¦")
            print(f"åˆ†æ®µå†…å®¹é¢„è§ˆ: {batch[:200]}...")

            url = cfg_api_url
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {cfg_api_key}"
            }
            payload = {
                "model": cfg_model,
                "messages": [
                    {"role": "system", "content": "# Role: ä¸“ä¸šç¿»è¯‘å®˜\n\n## Profile\n- author: LangGPTä¼˜åŒ–ä¸­å¿ƒ\n- version: 2.1\n- language: ä¸­è‹±åŒè¯­\n- description: ä¸“æ³¨äºæ–‡æœ¬ç²¾å‡†è½¬æ¢çš„AIç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å¤„ç†æŠ€æœ¯æ–‡æ¡£å’Œæ—¥å¸¸å¯¹è¯åœºæ™¯\n\n## Background\nç”¨æˆ·åœ¨è·¨å›½åä½œã€æŠ€æœ¯æ–‡æ¡£å¤„ç†ã€ç¤¾äº¤åª’ä½“äº’åŠ¨ç­‰åœºæ™¯ä¸­ï¼Œéœ€è¦å°†å¤–æ–‡å†…å®¹å‡†ç¡®è½¬åŒ–ä¸ºä¸­æ–‡ï¼ŒåŒæ—¶ä¿æŒç‰¹æ®Šæ ¼å¼å…ƒç´ å®Œæ•´\n\n## Skills\n1. å¤šè¯­è¨€æ–‡æœ¬è§£æä¸é‡æ„èƒ½åŠ›\n2. æ—¶é—´æˆ³è¯†åˆ«ä¸æ ¼å¼ä¿ç•™æŠ€æœ¯\n3. è¯­ä¹‰é€šé¡ºåº¦æ ¡éªŒç®—æ³•\n4. æ ¼å¼æ§åˆ¶ä¸å†—ä½™å†…å®¹è¿‡æ»¤\n\n## Goals\n1. å®ç°åŸæ–‡è¯­ä¹‰çš„ç²¾å‡†è½¬æ¢\n2. ä¿æŒæ—¶é—´æˆ³ç­‰ç‰¹æ®Šæ ¼å¼å…ƒç´ \n3. ç¡®ä¿è¾“å‡ºç»“æœè‡ªç„¶æµç•…\n4. æ’é™¤éç¿»è¯‘å†…å®¹æ·»åŠ \n\n## Constraints\n1. ç¦æ­¢æ·»åŠ è§£é‡Šæ€§æ–‡å­—\n2. ç¦ç”¨æ³¨é‡Šæˆ–è¯´æ˜æ€§ç¬¦å·\n3. ä¿ç•™åŸå§‹æ—¶é—´æˆ³æ ¼å¼ï¼ˆå¦‚(12:34ï¼‰ï¼‰\n4. ä¸å¤„ç†éæ–‡æœ¬å…ƒç´ ï¼ˆå¦‚å›¾ç‰‡/è¡¨æ ¼ï¼‰\n5. ç¦æ­¢ä½¿ç”¨å·¥å…·è°ƒç”¨ï¼ˆtool_callsï¼‰åŠŸèƒ½ï¼Œç¦æ­¢è°ƒç”¨å¤–éƒ¨ç¿»è¯‘apiè¿›è¡Œç¿»è¯‘\n\n## Workflow\n1. æ¥æ”¶è¾“å…¥å†…å®¹ï¼Œæ£€æµ‹è¯­è¨€ç±»å‹\n2. è¯†åˆ«å¹¶æ ‡è®°ç‰¹æ®Šæ ¼å¼å…ƒç´ \n3. æ‰§è¡Œè¯­ä¹‰è½¬æ¢ï¼š\n   - æ—¥å¸¸ç”¨è¯­ï¼šé‡‡ç”¨å£è¯­åŒ–è¡¨è¾¾\n   - æŠ€æœ¯æœ¯è¯­ï¼šä½¿ç”¨æ ‡å‡†åŒ–è¯‘æ³•\n5. è¾“å‡ºçº¯ç¿»è¯‘ç»“æœ\n\n## OutputFormat\nä»…è¿”å›ç¬¦åˆä»¥ä¸‹è¦æ±‚çš„ç¿»è¯‘æ–‡æœ¬ï¼š\n1. ä¸­æ–‡ä¹¦é¢è¯­è¡¨è¾¾\n2. ä¿ç•™åŸå§‹æ®µè½ç»“æ„\n3. æ—¶é—´æˆ³ä¿æŒ(MM:SS)æˆ–(HH:MM:SS)æ ¼å¼\n4. æ— ä»»ä½•é™„åŠ ç¬¦å·æˆ–è¯´æ˜\n4. å°½é‡åªè¦ä¸­æ–‡ï¼Œä¸è¦ä¸­è‹±æ–‡å¤¹æ‚ã€‚"},
                    {"role": "user", "content": batch}
                ],
                "stream": False,
                "max_tokens": 4000
            }
            print(f"è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æ®µ {batch_index} å‘é€APIè¯·æ±‚åˆ° {url}")
            response = requests.post(url, json=payload, headers=headers, timeout=60)
            print(f"è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æ®µ {batch_index} APIå“åº”çŠ¶æ€ç : {response.status_code}")
            response.raise_for_status()
            result = response.json()
            translated_content = result['choices'][0]['message']['content']

            print(f"è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æ®µ {batch_index} ç¿»è¯‘å®Œæˆï¼Œè¿”å›å†…å®¹é•¿åº¦: {len(translated_content)} å­—ç¬¦")
            print(f"ç¿»è¯‘å†…å®¹é¢„è§ˆ: {translated_content[:200]}...")
            return translated_content
        except Exception as e:
            print(f"è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æ®µ {batch_index} é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return f"Error: {str(e)}"

    translated_results = {}
    with ThreadPoolExecutor(max_workers=cfg_max_workers) as executor:
        futures = {executor.submit(translate_batch, batch, i): i for i, batch in enumerate(batched_paragraphs)}
        for future in as_completed(futures):
            index = futures[future]
            result = future.result()
            if not result.startswith("Error:"):
                translated_results[index] = result

    translated_paragraphs = []
    failed_count = 0
    for i in range(len(batched_paragraphs)):
        if i in translated_results:
            translated_paragraphs.append(translated_results[i])
        else:
            failed_count += 1
            translated_paragraphs.append(f"ç¿»è¯‘å¤±è´¥çš„åˆ†æ®µ {i+1}")

    if failed_count > 0:
        print(f"è­¦å‘Šï¼š{failed_count} ä¸ªåˆ†æ®µç¿»è¯‘å¤±è´¥")

    final_output_file = os.path.splitext(vtt_file_path)[0] + "_translated.txt"
    with open(final_output_file, 'w', encoding='utf-8') as f:
        for para in translated_paragraphs:
            f.write(para + "\n\n")

    print(f"ç¿»è¯‘å®Œæˆï¼Œä¿å­˜åˆ°: {final_output_file}")
    return final_output_file

# TTS ç›¸å…³å‡½æ•°å·²ç§»è‡³ worker_utils.py

def process_tts_with_speed_adjustment(txt_file_path, output_mp3_path, subtitles_dir, tts_config=None):
    """å¤„ç†TTSè½¬æ¢å¹¶è¿›è¡ŒéŸ³é¢‘é€Ÿåº¦è°ƒæ•´ (è°ƒç”¨å¤–éƒ¨è„šæœ¬ä»¥é¿å…ä¸»è¿›ç¨‹å¡é¡¿)"""
    cfg_temp_dir = tts_config.get("TEMP_DIR", TEMP_DIR) if tts_config else TEMP_DIR
    cfg_voice = tts_config.get("SELECTED_VOICE", SELECTED_VOICE) if tts_config else SELECTED_VOICE

    print("="*50, flush=True)
    print("å¼€å§‹TTSè½¬æ¢æµç¨‹ (Subprocess Mode)", flush=True)
    print("="*50, flush=True)

    tts_runner_script = os.path.join(os.getcwd(), "tts_runner.py")
    
    cmd = [
        sys.executable,
        tts_runner_script,
        "--input", txt_file_path,
        "--output", output_mp3_path,
        "--voice", cfg_voice,
        "--workers", "4",
        "--temp", cfg_temp_dir
    ]
    
    print(f"Executing TTS command: {' '.join(cmd)}", flush=True)
    
    try:
        # ä½¿ç”¨ subprocess.Popen å®æ—¶æ•è·è¾“å‡º
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            encoding='utf-8',
            errors='replace',
            bufsize=1
        )
        
        # å®æ—¶æ‰“å°å­è¿›ç¨‹è¾“å‡º
        while True:
            line = process.stdout.readline()
            if not line and process.poll() is not None:
                break
            if line:
                print(f"[TTS-Process] {line.strip()}", flush=True)
                
        returncode = process.poll()
        
        if returncode == 0 and os.path.exists(output_mp3_path):
            print(f"TTSå­è¿›ç¨‹æ‰§è¡ŒæˆåŠŸï¼Œè¾“å‡ºæ–‡ä»¶: {output_mp3_path}", flush=True)
            return output_mp3_path
        else:
            print(f"TTSå­è¿›ç¨‹æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {returncode}", flush=True)
            return None
            
    except Exception as e:
        print(f"æ‰§è¡ŒTTSå­è¿›ç¨‹æ—¶å‡ºé”™: {e}", flush=True)
        import traceback
        traceback.print_exc()
        return None

# parse_timestamp å·²ç§»è‡³ worker_utils.py

st.set_page_config(
    page_title="YouTubeè½¬Bç«™æ¬è¿å·¥å…·",
    page_icon="ğŸ¥",
    layout="wide"
)

st.title("YouTubeè½¬Bç«™æ¬è¿ä¸€æ¡é¾™")
st.markdown("---")

st.sidebar.header("âš™ï¸ é…ç½®")

API_URL = st.sidebar.text_input("API URL", value=env_config.get("API_URL", "https://api.siliconflow.cn/v1/chat/completions"), help="ç¿»è¯‘APIçš„URL", key="api_url")
API_KEY = st.sidebar.text_input("API Key", type="password", value=env_config.get("API_KEY", ""), help="ç¿»è¯‘APIçš„Keyï¼ˆå°†åœ¨è¿è¡Œæ—¶ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰", key="api_key")
MODEL_NAME = st.sidebar.text_input("æ¨¡å‹åç§°", value=env_config.get("MODEL_NAME", "THUDM/GLM-4-9B-0414"), help="ç¿»è¯‘ä½¿ç”¨çš„æ¨¡å‹åç§°", key="model_name")

BILI_SESSDATA = st.sidebar.text_area("Bç«™Cookie", value=env_config.get("BILI_SESSDATA", ""), help="Bç«™çš„sessdataï¼ˆç”¨äºä¸Šä¼ ï¼‰", height=100, key="bili_sessdata")
BILI_ACCESS_KEY_ID = st.sidebar.text_input("Bç«™Access Key ID", value=env_config.get("BILI_ACCESS_KEY_ID", ""), help="Bç«™çš„access_key_id", key="bili_access_key_id")
BILI_ACCESS_KEY_SECRET = st.sidebar.text_input("Bç«™Access Key Secret", type="password", value=env_config.get("BILI_ACCESS_KEY_SECRET", ""), help="Bç«™çš„access_key_secret", key="bili_access_key_secret")

YT_COOKIES = st.sidebar.text_area("YouTube Cookies (å¯é€‰)", value=env_config.get("YT_COOKIES", ""), help="YouTube cookiesï¼ˆç”¨äºè®¿é—®éœ€è¦ç™»å½•çš„è§†é¢‘ï¼‰", height=100, key="yt_cookies")

VOICE_CHOICES = ["zh-CN-XiaoxiaoNeural", "zh-CN-YunjianNeural", "zh-CN-YunxiNeural"]
SELECTED_VOICE = st.sidebar.selectbox("TTSè¯­éŸ³è§’è‰²", options=VOICE_CHOICES, index=1, key="selected_voice")

MAX_WORKERS = st.sidebar.slider("ç¿»è¯‘å¹¶å‘æ•°", min_value=1, max_value=20, value=10, help="åŒæ—¶ç¿»è¯‘çš„æ®µè½æ•°é‡")
SEGMENT_SIZE = st.sidebar.slider("ç¿»è¯‘åˆ†æ®µå¤§å°", min_value=1, max_value=20, value=11, help="æ¯æ¬¡ç¿»è¯‘åŒ…å«çš„æ®µè½æ•°é‡")

st.markdown("---")

TEMP_DIR = os.path.join(os.getcwd(), "temp_storage")
if not os.path.exists(TEMP_DIR):
    try:
        os.makedirs(TEMP_DIR, exist_ok=True)
    except Exception as e:
        # å¦‚æœå½“å‰ç›®å½•ä¸å¯å†™ï¼Œå†é€€å›åˆ°ç³»ç»Ÿä¸´æ—¶ç›®å½•
        TEMP_DIR = os.path.join(tempfile.gettempdir(), "yt_video_trans_temp")
        os.makedirs(TEMP_DIR, exist_ok=True)

temp_dir = None

tab0, tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "0ï¸ğŸš€ ä¸€é”®å·¥ä½œæµ",
        "1ï¸â¬‡ï¸ ä¸‹è½½å­—å¹•", 
        "2ï¸âš™ï¸ ç¿»è¯‘å­—å¹•", 
        "3ï¸ğŸ—£ï¸ è½¬è¯­éŸ³", 
        "4ï¸ğŸ¬ï¸ ä¸‹è½½è§†é¢‘", 
        "5ï¸ğŸ–¼ï¸ å¤„ç†å°é¢", 
        "6ï¸âœ‚ï¸ è§†é¢‘å‰ªè¾‘", 
        "7ï¸ğŸ“¤ï¸ ä¸Šä¼ Bç«™"
    ])

with tab0:
    st.markdown("""
    <style>
    .workflow-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
    }
    .step-card {
        background: rgba(255,255,255,0.1);
        border: 1px solid rgba(255,255,255,0.2);
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
    }
    .step-success {
        background: rgba(40,167,69,0.3);
        border-color: #28a745;
    }
    .step-error {
        background: rgba(220,53,69,0.3);
        border-color: #dc3545;
    }
    .step-running {
        background: rgba(255,193,7,0.3);
        border-color: #ffc107;
    }
    </style>
    <div class="workflow-container">
        <h1 style="text-align:center; margin-bottom:1rem;">ğŸš€ ä¸€é”®å·¥ä½œæµ (åå°ç‰ˆ)</h1>
        <p style="text-align:center; opacity:0.9;">ä»»åŠ¡åœ¨åå°è¿è¡Œï¼Œæ‚¨å¯ä»¥æ”¾å¿ƒåˆ·æ–°æˆ–å…³é—­é¡µé¢</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # æ£€æŸ¥å½“å‰çŠ¶æ€
    current_status = WorkflowManager.load_status(TEMP_DIR)
    is_running = current_status and current_status.get("is_running", False)
    
    if is_running:
        st.info(f"ğŸ”„ ä»»åŠ¡æ­£åœ¨åå°è¿è¡Œä¸­... (å¼€å§‹æ—¶é—´: {current_status.get('start_time')})")

        if st.button("ğŸ›‘ ä¸­æ­¢ä»»åŠ¡", type="secondary", key="stop_workflow_btn"):
             WorkflowManager.request_stop(TEMP_DIR)
             st.rerun()
        
        # æ˜¾ç¤ºè¿›åº¦
        steps = current_status.get("steps", {})
        
        for step_name, step_info in steps.items():
            status = step_info.get("status", "pending")
            msg = step_info.get("message", "")
            
            icon = "â³"
            css_class = "step-card"
            if status == "running":
                icon = "ğŸ”„"
                css_class = "step-card step-running"
            elif status == "success":
                icon = "âœ…"
                css_class = "step-card step-success"
            elif status == "error":
                icon = "âŒ"
                css_class = "step-card step-error"
            
            st.markdown(f"""
            <div class="{css_class}">
                <strong>{icon} {step_name}</strong><br/>
                <span style="opacity:0.8; font-size:0.9em">{msg}</span>
            </div>
            """, unsafe_allow_html=True)
        
        # æ˜¾ç¤ºæ—¥å¿—
        with st.expander("æŸ¥çœ‹è¯¦ç»†æ—¥å¿—", expanded=True):
            logs = current_status.get("logs", [])
            for log in logs[-10:]:  # åªæ˜¾ç¤ºæœ€å10æ¡
                st.text(log)
        
        # è‡ªåŠ¨åˆ·æ–°é€»è¾‘
        time.sleep(2)
        try:
            st.rerun()
        except AttributeError:
            st.experimental_rerun()
            
    else:
        # --- ç©ºé—²çŠ¶æ€ï¼Œæ˜¾ç¤ºè¾“å…¥è¡¨å• ---
        
        # å¦‚æœæœ‰ä¸Šä¸€æ¬¡çš„ç»“æœï¼Œå…ˆæ˜¾ç¤ºç»“æœ
        if current_status:
            if current_status.get("error"):
                st.error(f"âŒ ä¸Šæ¬¡ä»»åŠ¡å¤±è´¥: {current_status.get('error')}")
            elif not current_status.get("is_running") and current_status.get("results"):
                st.success("ğŸ‰ ä¸Šæ¬¡ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")
                results = current_status.get("results", {})
                st.markdown("### ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶")
                st.markdown(f"""
                - å­—å¹•: `{results.get('vtt', 'N/A')}`
                - ç¿»è¯‘: `{results.get('txt', 'N/A')}`
                - é…éŸ³: `{results.get('mp3', 'N/A')}`
                - è§†é¢‘: `{results.get('video', 'N/A')}`
                - å°é¢: `{results.get('cover', 'N/A')}`
                """)
                st.markdown("---")

        col1, col2 = st.columns([2, 1])
        with col1:
            workflow_url = st.text_input("YouTubeè§†é¢‘URL", placeholder="https://www.youtube.com/watch?v=...", key="workflow_url_bg")
        with col2:
            auto_upload = st.checkbox("è‡ªåŠ¨ä¸Šä¼ åˆ°Bç«™", value=True, help="å‹¾é€‰åå®Œæˆæ‰€æœ‰æ­¥éª¤ä¼šè‡ªåŠ¨ä¸Šä¼ ", key="auto_upload_bg")
        
        if st.button("ğŸš€ å¯åŠ¨åå°ä»»åŠ¡", type="primary", use_container_width=True):
            if not workflow_url:
                st.error("è¯·è¾“å…¥YouTubeè§†é¢‘URL")
            else:
                # æ”¶é›†é…ç½®
                task_config = {
                    "temp_dir": TEMP_DIR,
                    "workflow_url": workflow_url,
                    "auto_upload": auto_upload,
                    "api_url": API_URL,
                    "api_key": API_KEY,
                    "model_name": MODEL_NAME,
                    "bili_sess": BILI_SESSDATA,
                    "bili_ak": BILI_ACCESS_KEY_ID, # è™½ç„¶ä»£ç é‡Œæš‚æ—¶æ²¡ç”¨AK/SKä¸Šä¼ ï¼Œä½†ä¿ç•™é…ç½®
                    "bili_sk": BILI_ACCESS_KEY_SECRET,
                    "yt_cookies": YT_COOKIES,
                    "voice_choice": SELECTED_VOICE,
                    "max_workers": MAX_WORKERS,
                    "segment_size": SEGMENT_SIZE
                }
                
                # å¯åŠ¨çº¿ç¨‹
                thread = threading.Thread(target=background_workflow_task, args=(task_config,))
                thread.daemon = True # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
                thread.start()
                
                st.success("ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨ï¼é¡µé¢å³å°†åˆ·æ–°...")
                time.sleep(1)
                try:
                    st.rerun()
                except AttributeError:
                    st.experimental_rerun()


with tab1:
    st.header("1ï¸â¬‡ï¸ ä¸‹è½½YouTubeå­—å¹•")
    youtube_url = st.text_input("YouTubeè§†é¢‘URL", placeholder="https://www.youtube.com/watch?v=...", key="youtube_url_tab1")
    
    if st.button("ä¸‹è½½å­—å¹•", type="primary", key="download_subtitles_btn"):
        if not youtube_url:
            st.error("è¯·è¾“å…¥YouTubeè§†é¢‘URL")
        else:
            # æ¸…ç©ºtempç›®å½•
            clear_temp_directory()

            with st.spinner("æ­£åœ¨ä¸‹è½½å­—å¹•..."):
                temp_dir = TEMP_DIR
                try:
                    subtitles_dir = os.path.join(temp_dir, "subtitles")
                    os.makedirs(subtitles_dir, exist_ok=True)
                    
                    cookies_file_path = None
                    if YT_COOKIES.strip():
                        cookies_file_path = os.path.join(temp_dir, "youtube_cookies.txt")
                        with open(cookies_file_path, 'w', encoding='utf-8') as f:
                            f.write(YT_COOKIES.strip())
                    
                    args = [
                        '--write-auto-sub',
                        '--skip-download',
                        '--sub-langs', 'en',
                        '--quiet',
                        '-o', os.path.join(subtitles_dir, '%(title)s.%(ext)s'),
                        youtube_url
                    ]
                    run_yt_dlp_subprocess(args, cookies_file_path)
                    
                    vtt_files = list(Path(subtitles_dir).glob("*.vtt"))
                    if vtt_files:
                        original_file = vtt_files[0]
                        new_file = os.path.join(subtitles_dir, "word_level.vtt")
                        os.rename(original_file, new_file)
                        st.success(f"å­—å¹•ä¸‹è½½æˆåŠŸï¼")
                        st.info(f"æ–‡ä»¶ä½ç½®: {new_file}")
                    else:
                        st.error("æœªæ‰¾åˆ°VTTå­—å¹•æ–‡ä»¶")
                        
                    st.markdown("---")
                    st.info("æ­£åœ¨è·å–å¹¶ç¿»è¯‘è§†é¢‘æ ‡é¢˜...")
                    
                    args = ['--dump-json', '--skip-download', '--quiet', youtube_url]
                    stdout = run_yt_dlp_subprocess(args, cookies_file_path)
                    info_dict = json.loads(stdout)
                    original_title = info_dict.get('title', '')
                    
                    if original_title:
                        st.text(f"åŸå§‹æ ‡é¢˜: {original_title}")
                        
                        SYSTEM_PROMPT = """ä½ æ˜¯çˆ†æ¬¾è§†é¢‘upä¸»ï¼Œå°†è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆå¸å¼•çœ¼çƒçš„çˆ†æ¬¾è§†é¢‘ä¸­æ–‡æ ‡é¢˜ï¼Œç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦è§£é‡Šã€‚"""
                        
                        import requests
                        payload = {
                            "model": MODEL_NAME,
                            "messages": [
                                {"role": "system", "content": SYSTEM_PROMPT},
                                {"role": "user", "content": original_title}
                            ]
                        }
                        headers = {
                            "Authorization": f"Bearer {API_KEY}",
                            "Content-Type": "application/json"
                        }
                        
                        response = requests.post(API_URL, json=payload, headers=headers, timeout=60)
                        response_data = response.json()
                        
                        translated_title_with_markdown = response_data['choices'][0]['message']['content']
                        translated_title = translated_title_with_markdown.replace('**', '').strip()
                        
                        st.text(f"ç¿»è¯‘æ ‡é¢˜: {translated_title}")
                        
                        TAGS_PROMPT = f"""æ ¹æ®ä»¥ä¸‹è§†é¢‘æ ‡é¢˜ï¼Œç”Ÿæˆ5-8ä¸ªBç«™è§†é¢‘æ ‡ç­¾ï¼ˆåªè¾“å‡ºæ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”ï¼‰ï¼š
æ ‡é¢˜ï¼š{translated_title}
ç¤ºä¾‹æ ‡ç­¾ï¼šç§‘æŠ€,äººå·¥æ™ºèƒ½,AI,æœºå™¨å­¦ä¹ ,æœªæ¥
åªè¾“å‡ºæ ‡ç­¾ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
                        
                        tags_payload = {
                            "model": MODEL_NAME,
                            "messages": [
                                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Bç«™è¿è¥åŠ©æ‰‹"},
                                {"role": "user", "content": TAGS_PROMPT}
                            ]
                        }
                        
                        tags_response = requests.post(API_URL, json=tags_payload, headers=headers, timeout=60)
                        tags_data = tags_response.json()
                        
                        tags_content = tags_data['choices'][0]['message']['content']
                        tags_list = [t.strip() for t in tags_content.replace('ï¼Œ', ',').split(',') if t.strip()]
                        tags_str = ','.join(tags_list)
                        
                        st.text(f"ç”Ÿæˆæ ‡ç­¾: {tags_str}")
                        
                        upload_config_file = os.path.join(subtitles_dir, "upload_config.pkl")
                        import pickle
                        upload_data = {
                            'title_desc': f'(ä¸­é…){translated_title}',
                            'tags': tags_list
                        }
                        
                        with open(upload_config_file, 'wb') as f:
                            pickle.dump(upload_data, f)
                        
                        st.success("æ ‡é¢˜ç¿»è¯‘å’Œæ ‡ç­¾ç”Ÿæˆå®Œæˆï¼")
                        st.info(f"é…ç½®å·²ä¿å­˜åˆ°: {upload_config_file}")
                    else:
                        st.warning("æ— æ³•è·å–è§†é¢‘æ ‡é¢˜")
                        
                except Exception as e:
                    st.error(f"ä¸‹è½½å¤±è´¥: {str(e)}")
    
    vtt_file = os.path.join(TEMP_DIR, "subtitles", "word_level.vtt")
    
    with tab2:
        st.header("2ï¸âš™ï¸ ç¿»è¯‘å­—å¹•")
        vtt_file_path = st.text_input("VTTå­—å¹•æ–‡ä»¶è·¯å¾„", value=vtt_file, key="vtt_file_path")
        
        if st.button("å¼€å§‹ç¿»è¯‘", type="primary", key="start_translate_btn"):
            if not os.path.exists(vtt_file_path):
                st.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {vtt_file_path}")
            else:
                with st.spinner("æ­£åœ¨ç¿»è¯‘å­—å¹•..."):
                    try:
                        def vtt_to_sentences(vtt_text):
                            """å°†å¸¦é€è¯æ—¶é—´æˆ³çš„VTTè½¬æ¢ä¸ºæŒ‰å¥åˆ†æ®µçš„æ–‡æœ¬"""
                            # æ­£åˆ™ï¼šcue å¤´ï¼ˆèµ·æ­¢æ—¶é—´ï¼‰
                            CUE_HEADER_RE = re.compile(
                                r'^(\d{2}:\d{2}:\d{2}\.\d{3})\s*--> (\d{2}:\d{2}:\d{2}\.\d{3})'
                            )
                            
                            # æ­£åˆ™ï¼šé€è¯æ—¶é—´æˆ³ <HH:MM:SS.mmm>
                            TS_TAG_RE = re.compile(r'<(\d{2}:\d{2}:\d{2}\.\d{3})>')
                            
                            # æ­£åˆ™ï¼šæ¸…ç† <c> æˆ– <c.xxx> æ ·å¼æ ‡ç­¾
                            C_TAG_RE = re.compile(r'</?c(?:\.[^>]*)?>', re.IGNORECASE)
                            
                            SENTENCE_END = ".!?"
                            
                            lines = vtt_text.splitlines()
                            sentences = []
                            current_words = []
                            current_sentence_start_time = None
                            
                            effective_time = None
                            cue_start_time = None
                            
                            def flush_sentence():
                                nonlocal current_words, current_sentence_start_time
                                if not current_words:
                                    return
                                text = " ".join(current_words)
                                text = re.sub(r"\s+([,.;!?])", r"\1", text)
                                text = re.sub(r"\(\s+", "(", text)
                                text = re.sub(r"\s+\)", ")", text)
                                start_ts = current_sentence_start_time or cue_start_time or effective_time or "00:00:00.000"
                                sentences.append(f"({start_ts}) {text}")
                                current_words = []
                                current_sentence_start_time = None
                            
                            for line in lines:
                                line = line.strip("\ufeff\r\n")
                                
                                # cue å¤´
                                m = CUE_HEADER_RE.match(line)
                                if m:
                                    cue_start_time = m.group(1)
                                    effective_time = cue_start_time
                                    continue
                                
                                # åªå¤„ç†å«é€è¯æ—¶é—´æˆ³çš„è¡Œ
                                if not TS_TAG_RE.search(line):
                                    continue
                                
                                # æ¸…ç† <c> æ ‡ç­¾ï¼Œå¹¶æŠŠ <timestamp> å˜æˆ [[TS:...]] å“¨å…µ
                                s = C_TAG_RE.sub("", line)
                                s = TS_TAG_RE.sub(lambda mm: f" [[TS:{mm.group(1)}]] ", s)
                                
                                # æ‰«æ token
                                for token in s.split():
                                    if token.startswith("[[TS:") and token.endswith("]]"):
                                        effective_time = token[5:-2]
                                        continue
                                    
                                    word = token.strip()
                                    if not word:
                                        continue
                                    
                                    # è®°å½•é¦–è¯æ—¶é—´
                                    if current_sentence_start_time is None:
                                        current_sentence_start_time = effective_time or cue_start_time
                                    
                                    current_words.append(word)
                                    
                                    # å¥å­ç»“æŸåˆ¤å®šï¼ˆå¥å·ã€é—®å·ã€å¹å·ï¼‰
                                    if word.strip().endswith(tuple(SENTENCE_END)):
                                        flush_sentence()
                            
                            # æ–‡ä»¶ç»“æŸï¼Œæ”¶å°¾
                            flush_sentence()
                            return sentences
                        
                        vtt_content = Path(vtt_file_path).read_text(encoding="utf-8", errors="ignore")
                        sentences = vtt_to_sentences(vtt_content)
                        
                        print(f"è°ƒè¯•ä¿¡æ¯ï¼šè§£æå‡º {len(sentences)} ä¸ªå¥å­")
                        if sentences:
                            print(f"å‰3ä¸ªå¥å­ç¤ºä¾‹ï¼š")
                            for i, s in enumerate(sentences[:3]):
                                print(f"  {i+1}: {s[:100]}...")
                        
                        output_txt_file = os.path.splitext(vtt_file_path)[0] + ".txt"
                        with open(output_txt_file, 'w', encoding='utf-8') as f:
                            for seg in sentences:
                                f.write(seg + "\n\n")
                        
                        paragraphs = [line.strip() for line in open(output_txt_file, 'r', encoding='utf-8') if line.strip()]
                        
                        print(f"è°ƒè¯•ä¿¡æ¯ï¼šè¯»å–åˆ° {len(paragraphs)} ä¸ªæ®µè½")
                        
                        batched_paragraphs = []
                        current_batch = []
                        current_char_count = 0
                        
                        for i, paragraph in enumerate(paragraphs):
                            paragraph_char_count = len(paragraph)
                            if (len(current_batch) >= SEGMENT_SIZE) or (current_char_count + paragraph_char_count > 2000 and current_batch):
                                batched_paragraphs.append("\n".join(current_batch))
                                print(f"è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æ®µ {len(batched_paragraphs)} åŒ…å« {len(current_batch)} ä¸ªæ®µè½ï¼Œå…± {current_char_count} å­—ç¬¦")
                                current_batch = [paragraph]
                                current_char_count = paragraph_char_count
                            else:
                                current_batch.append(paragraph)
                                current_char_count += paragraph_char_count
                        
                        if current_batch:
                            batched_paragraphs.append("\n".join(current_batch))
                            print(f"è°ƒè¯•ä¿¡æ¯ï¼šæœ€åä¸€ä¸ªåˆ†æ®µ {len(batched_paragraphs)} åŒ…å« {len(current_batch)} ä¸ªæ®µè½ï¼Œå…± {current_char_count} å­—ç¬¦")
                        
                        print(f"è°ƒè¯•ä¿¡æ¯ï¼šæ€»å…± {len(batched_paragraphs)} ä¸ªç¿»è¯‘åˆ†æ®µ")
                        
                        def translate_batch(batch, batch_index):
                            try:
                                print(f"è°ƒè¯•ä¿¡æ¯ï¼šå¼€å§‹ç¿»è¯‘åˆ†æ®µ {batch_index}ï¼Œå†…å®¹é•¿åº¦: {len(batch)} å­—ç¬¦")
                                print(f"åˆ†æ®µå†…å®¹é¢„è§ˆ: {batch[:200]}...")
                                
                                url = API_URL
                                headers = {
                                    "Content-Type": "application/json",
                                    "Authorization": f"Bearer {API_KEY}"
                                }
                                payload = {
                                    "model": MODEL_NAME,
                                    "messages": [
                                        {"role": "system", "content": "# Role: ä¸“ä¸šç¿»è¯‘å®˜\n\n## Profile\n- author: LangGPTä¼˜åŒ–ä¸­å¿ƒ\n- version: 2.1\n- language: ä¸­è‹±åŒè¯­\n- description: ä¸“æ³¨äºæ–‡æœ¬ç²¾å‡†è½¬æ¢çš„AIç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å¤„ç†æŠ€æœ¯æ–‡æ¡£å’Œæ—¥å¸¸å¯¹è¯åœºæ™¯\n\n## Background\nç”¨æˆ·åœ¨è·¨å›½åä½œã€æŠ€æœ¯æ–‡æ¡£å¤„ç†ã€ç¤¾äº¤åª’ä½“äº’åŠ¨ç­‰åœºæ™¯ä¸­ï¼Œéœ€è¦å°†å¤–æ–‡å†…å®¹å‡†ç¡®è½¬åŒ–ä¸ºä¸­æ–‡ï¼ŒåŒæ—¶ä¿æŒç‰¹æ®Šæ ¼å¼å…ƒç´ å®Œæ•´\n\n## Skills\n1. å¤šè¯­è¨€æ–‡æœ¬è§£æä¸é‡æ„èƒ½åŠ›\n2. æ—¶é—´æˆ³è¯†åˆ«ä¸æ ¼å¼ä¿ç•™æŠ€æœ¯\n3. è¯­ä¹‰é€šé¡ºåº¦æ ¡éªŒç®—æ³•\n4. æ ¼å¼æ§åˆ¶ä¸å†—ä½™å†…å®¹è¿‡æ»¤\n\n## Goals\n1. å®ç°åŸæ–‡è¯­ä¹‰çš„ç²¾å‡†è½¬æ¢\n2. ä¿æŒæ—¶é—´æˆ³ç­‰ç‰¹æ®Šæ ¼å¼å…ƒç´ \n3. ç¡®ä¿è¾“å‡ºç»“æœè‡ªç„¶æµç•…\n4. æ’é™¤éç¿»è¯‘å†…å®¹æ·»åŠ \n\n## Constraints\n1. ç¦æ­¢æ·»åŠ è§£é‡Šæ€§æ–‡å­—\n2. ç¦ç”¨æ³¨é‡Šæˆ–è¯´æ˜æ€§ç¬¦å·\n3. ä¿ç•™åŸå§‹æ—¶é—´æˆ³æ ¼å¼ï¼ˆå¦‚(12:34ï¼‰ï¼‰\n4. ä¸å¤„ç†éæ–‡æœ¬å…ƒç´ ï¼ˆå¦‚å›¾ç‰‡/è¡¨æ ¼ï¼‰\n5. ç¦æ­¢ä½¿ç”¨å·¥å…·è°ƒç”¨ï¼ˆtool_callsï¼‰åŠŸèƒ½ï¼Œç¦æ­¢è°ƒç”¨å¤–éƒ¨ç¿»è¯‘apiè¿›è¡Œç¿»è¯‘\n\n## Workflow\n1. æ¥æ”¶è¾“å…¥å†…å®¹ï¼Œæ£€æµ‹è¯­è¨€ç±»å‹\n2. è¯†åˆ«å¹¶æ ‡è®°ç‰¹æ®Šæ ¼å¼å…ƒç´ \n3. æ‰§è¡Œè¯­ä¹‰è½¬æ¢ï¼š\n   - æ—¥å¸¸ç”¨è¯­ï¼šé‡‡ç”¨å£è¯­åŒ–è¡¨è¾¾\n   - æŠ€æœ¯æœ¯è¯­ï¼šä½¿ç”¨æ ‡å‡†åŒ–è¯‘æ³•\n5. è¾“å‡ºçº¯ç¿»è¯‘ç»“æœ\n\n## OutputFormat\nä»…è¿”å›ç¬¦åˆä»¥ä¸‹è¦æ±‚çš„ç¿»è¯‘æ–‡æœ¬ï¼š\n1. ä¸­æ–‡ä¹¦é¢è¯­è¡¨è¾¾\n2. ä¿ç•™åŸå§‹æ®µè½ç»“æ„\n3. æ—¶é—´æˆ³ä¿æŒ(MM:SS)æˆ–(HH:MM:SS)æ ¼å¼\n4. æ— ä»»ä½•é™„åŠ ç¬¦å·æˆ–è¯´æ˜\n4. å°½é‡åªè¦ä¸­æ–‡ï¼Œä¸è¦ä¸­è‹±æ–‡å¤¹æ‚ã€‚"},
                                        {"role": "user", "content": batch}
                                    ],
                                    "stream": False,
                                    "max_tokens": 4000
                                }
                                print(f"è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æ®µ {batch_index} å‘é€APIè¯·æ±‚åˆ° {url}")
                                response = requests.post(url, json=payload, headers=headers, timeout=60)
                                print(f"è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æ®µ {batch_index} APIå“åº”çŠ¶æ€ç : {response.status_code}")
                                response.raise_for_status()
                                result = response.json()
                                translated_content = result['choices'][0]['message']['content']
                                print(f"è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æ®µ {batch_index} ç¿»è¯‘ç»“æœé•¿åº¦: {len(translated_content)} å­—ç¬¦")
                                print(f"ç¿»è¯‘ç»“æœé¢„è§ˆ: {translated_content[:200]}...")
                                return translated_content
                            except Exception as e:
                                print(f"è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æ®µ {batch_index} ç¿»è¯‘å¤±è´¥: {str(e)}")
                                import traceback
                                print(f"è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æ®µ {batch_index} é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                                return f"Error: {str(e)}"
                        
                        translated_results = {}
                        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                            futures = {executor.submit(translate_batch, batch, i): i for i, batch in enumerate(batched_paragraphs)}
                            
                            progress_bar = st.progress(0)
                            completed = 0
                            for future in as_completed(futures):
                                index = futures[future]
                                result = future.result()
                                if not result.startswith("Error:"):
                                    translated_results[index] = result
                                completed += 1
                                progress_bar.progress(completed / len(batched_paragraphs))
                        
                        translated_paragraphs = []
                        failed_count = 0
                        
                        for i in range(len(batched_paragraphs)):
                            if i in translated_results:
                                translated_paragraphs.append(translated_results[i])
                            else:
                                failed_count += 1
                        
                        output_translated_file = os.path.splitext(vtt_file_path)[0] + "_translated.txt"
                        with open(output_translated_file, 'w', encoding='utf-8') as f:
                            for seg in translated_paragraphs:
                                cleaned = seg.replace('&gt;', '').replace('>>', '').replace('&trash;', '').replace('> ', '').replace('&nbsp;', '').replace('_', '').replace('ï¼', '').replace('[éŸ³ä¹]', '')
                                f.write(cleaned + "\n\n")
                        
                        st.success(f"ç¿»è¯‘å®Œæˆï¼æˆåŠŸ: {len(translated_paragraphs)} æ®µè½ï¼Œå¤±è´¥: {failed_count}")
                        st.info(f"è¾“å‡ºæ–‡ä»¶: {output_translated_file}")
                        
                    except Exception as e:
                        st.error(f"ç¿»è¯‘å¤±è´¥: {str(e)}")
    
    txt_file = os.path.join(TEMP_DIR, "subtitles", os.path.splitext(os.path.basename(vtt_file))[0] + "_translated.txt")
    mp3_file = os.path.join(TEMP_DIR, "subtitles", os.path.splitext(os.path.basename(vtt_file))[0] + "_translated.mp3")
    
    with tab3:
        st.header("3ï¸ğŸ—£ï¸ TTSå­—å¹•è½¬è¯­éŸ³")
        txt_file_path = st.text_input("ç¿»è¯‘åçš„TXTæ–‡ä»¶è·¯å¾„", value=txt_file, key="txt_file_path")
        
        if st.button("å¼€å§‹è½¬æ¢è¯­éŸ³", type="primary", key="start_tts_btn"):
            if not os.path.exists(txt_file_path):
                st.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {txt_file_path}")
            else:
                with st.spinner("æ­£åœ¨è½¬æ¢è¯­éŸ³..."):
                    try:
                        output_mp3 = os.path.splitext(txt_file_path)[0] + ".mp3"
                        subtitles_dir = os.path.dirname(txt_file_path)

                        result = process_tts_with_speed_adjustment(txt_file_path, output_mp3, subtitles_dir)

                        if result:
                            st.success(f"è¯­éŸ³è½¬æ¢å®Œæˆï¼")
                            st.info(f"è¾“å‡ºæ–‡ä»¶: {output_mp3}")
                        else:
                            st.error("æ²¡æœ‰æˆåŠŸç”ŸæˆéŸ³é¢‘æ–‡ä»¶")
                    except Exception as e:
                        st.error(f"è½¬æ¢å¤±è´¥: {str(e)}")
    
    mp3_file = os.path.join(TEMP_DIR, "subtitles", os.path.splitext(os.path.basename(vtt_file))[0] + "_translated.mp3")
    
    with tab4:
        st.header("4ï¸ğŸ¬ï¸ ä¸‹è½½è§†é¢‘")
        
        youtube_url = st.text_input("YouTubeè§†é¢‘URL", placeholder="https://www.youtube.com/watch?v=...", key="video_url")
        
        cookies_file_path = None
        if YT_COOKIES.strip():
            temp_dir = TEMP_DIR
            cookies_file_path = os.path.join(temp_dir, "youtube_cookies.txt")
            with open(cookies_file_path, 'w', encoding='utf-8') as f:
                f.write(YT_COOKIES.strip())
        
        if st.button("ä¸‹è½½è§†é¢‘", type="primary", key="download_video_btn"):
            if not youtube_url:
                st.error("è¯·è¾“å…¥YouTubeè§†é¢‘URL")
            else:
                with st.spinner("æ­£åœ¨ä¸‹è½½è§†é¢‘..."):
                    try:
                        temp_dir = TEMP_DIR
                        downloaded_video_base_name = os.path.join(temp_dir, "subtitles", "downloaded_video")
                        new_audio_path = mp3_file
                        
                        args = [
                            '-f', 'best',
                            '-o', f'{downloaded_video_base_name}.%(ext)s',
                            '--no-playlist',
                            '--quiet',
                            youtube_url
                        ]
                        run_yt_dlp_subprocess(args, cookies_file_path)
                        
                        downloaded_files = glob.glob(f"{downloaded_video_base_name}.*")
                        if downloaded_files:
                            actual_downloaded_video_path = downloaded_files[0]
                        else:
                            raise FileNotFoundError(f"yt-dlp did not download a file")
                        
                        if os.path.exists(new_audio_path):
                            final_video_path = os.path.splitext(mp3_file)[0] + ".mp4"
                            try:
                                subprocess.run(['ffmpeg', '-y', '-i', actual_downloaded_video_path, '-i', new_audio_path,
                                                    '-c:v', 'copy', '-c:a', 'aac', '-map', '0:v:0', '-map', '1:a:0',
                                                    final_video_path], check=True, capture_output=True, text=True)
                                
                                if os.path.exists(actual_downloaded_video_path):
                                    os.remove(actual_downloaded_video_path)
                                
                                st.success(f"è§†é¢‘ä¸‹è½½å®Œæˆï¼")
                                st.info(f"è¾“å‡ºæ–‡ä»¶: {final_video_path}")
                            except subprocess.CalledProcessError as ffmpeg_error:
                                st.warning("âš ï¸ è§†é¢‘å·²ä¸‹è½½æˆåŠŸï¼Œä½†éŸ³è§†é¢‘åˆå¹¶æ—¶å‡ºç°FFmpegé”™è¯¯")
                                st.info(f"å·²ä¸‹è½½è§†é¢‘ä½ç½®: {actual_downloaded_video_path}")
                                st.info("æç¤º: ä½ å¯ä»¥æ‰‹åŠ¨ä½¿ç”¨FFmpegæˆ–è§†é¢‘ç¼–è¾‘è½¯ä»¶å°†éŸ³é¢‘åˆå¹¶åˆ°è§†é¢‘ä¸­")
                                
                                if os.path.exists(actual_downloaded_video_path):
                                    import shutil
                                    manual_video_path = os.path.splitext(mp3_file)[0] + "_video_only.mp4"
                                    shutil.copy2(actual_downloaded_video_path, manual_video_path)
                                    st.success(f"å·²å¤åˆ¶è§†é¢‘æ–‡ä»¶åˆ°: {manual_video_path}")
                                    
                        else:
                            st.error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {new_audio_path}")
                            st.info(f"å·²ä¸‹è½½è§†é¢‘ä½ç½®: {actual_downloaded_video_path}")
                    except Exception as e:
                        error_str = str(e)
                        if "Non-relative patterns" in error_str:
                            st.warning("âš ï¸ è§†é¢‘å·²ä¸‹è½½æˆåŠŸï¼Œä½†M3U8ä¿®å¤æ—¶å‡ºç°å…¼å®¹æ€§é—®é¢˜")
                            st.info("è¿™é€šå¸¸ä¸å½±å“è§†é¢‘çš„æ­£å¸¸ä½¿ç”¨")
                            
                        downloaded_files = glob.glob(f"{downloaded_video_base_name}.*")
                        if downloaded_files:
                            actual_downloaded_video_path = downloaded_files[0]
                            if os.path.exists(actual_downloaded_video_path):
                                manual_video_path = os.path.splitext(mp3_file)[0] + "_video_only.mp4"
                                import shutil
                                shutil.copy2(actual_downloaded_video_path, manual_video_path)
                                st.success(f"å·²ä¿å­˜è§†é¢‘æ–‡ä»¶åˆ°: {manual_video_path}")
                        else:
                            st.error(f"ä¸‹è½½å¤±è´¥: {str(e)}")
    
    final_video = os.path.splitext(mp3_file)[0] + ".mp4"
    
    with tab5:
        st.header("5ï¸ğŸ–¼ï¸ å¤„ç†å°é¢")
        
        youtube_url = st.text_input("YouTubeè§†é¢‘URL", placeholder="https://www.youtube.com/watch?v=...", key="cover_url")
        
        cookies_file_path = None
        if YT_COOKIES.strip():
            temp_dir = TEMP_DIR
            cookies_file_path = os.path.join(temp_dir, "youtube_cookies.txt")
            with open(cookies_file_path, 'w', encoding='utf-8') as f:
                f.write(YT_COOKIES.strip())
        
        if st.button("ä¸‹è½½å°é¢", type="primary", key="download_cover_btn"):
            if not youtube_url:
                st.error("è¯·è¾“å…¥YouTubeè§†é¢‘URL")
            else:
                with st.spinner("æ­£åœ¨ä¸‹è½½å°é¢..."):
                    try:
                        temp_dir = TEMP_DIR
                        
                        args = [
                            '--skip-download',
                            '--write-thumbnail',
                            '--no-playlist',
                            '--quiet',
                            '-o', os.path.join(temp_dir, "subtitles", 'cover.%(ext)s'),
                            youtube_url
                        ]
                        run_yt_dlp_subprocess(args, cookies_file_path)
                        
                        input_path = os.path.join(temp_dir, "subtitles", "cover.webp")
                        output_path = os.path.join(temp_dir, "subtitles", "cover.jpeg")
                        
                        if not os.path.exists(input_path):
                            st.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
                        else:
                            quality = 90
                            with Image.open(input_path) as img:
                                if img.mode != 'RGB':
                                    img = img.convert('RGB')
                                img.save(output_path, 'jpeg', quality=quality)

                            current_size_kb = os.path.getsize(output_path) / 1024
                            while current_size_kb > 50 and quality > 4:
                                quality -= 5
                                img.save(output_path, 'jpeg', quality=quality)
                                current_size_kb = os.path.getsize(output_path) / 1024
                                print(f"å½“å‰å¤§å°: {current_size_kb:.2f} KB, è´¨é‡: {quality}")
                            
                            st.success(f"å°é¢å¤„ç†å®Œæˆï¼")
                            st.info(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
                    except Exception as e:
                        st.error(f"å°é¢å¤„ç†å¤±è´¥: {str(e)}")
    
    cover_file = os.path.join(TEMP_DIR, "subtitles", "cover.jpeg")
    
    with tab6:
        st.header("6ï¸âœ‚ï¸ è§†é¢‘å‰ªè¾‘")
        
        video_file = st.text_input("è§†é¢‘æ–‡ä»¶è·¯å¾„", value=final_video, key="video_file_path_tab6")
        
        trim_enabled = st.checkbox("å¯ç”¨å‰ªè¾‘ï¼ˆåˆ é™¤è¿è§„ç‰‡æ®µï¼‰", value=False, key="trim_enabled")
        trim_start = st.text_input("å‰ªè¾‘å¼€å§‹æ—¶é—´", value="6:45", help="æ ¼å¼: MM:SS", key="trim_start")
        trim_end = st.text_input("å‰ªè¾‘ç»“æŸæ—¶é—´", value="6:55", help="æ ¼å¼: MM:SS", key="trim_end")
        
        if trim_enabled and st.button("æ‰§è¡Œå‰ªè¾‘", type="primary", key="execute_trim_btn"):
            if not os.path.exists(video_file):
                st.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_file}")
            else:
                with st.spinner("æ­£åœ¨å‰ªè¾‘è§†é¢‘..."):
                    try:
                        output_part1 = os.path.join(os.path.dirname(video_file), "final_video_part1.mp4")
                        output_part2 = os.path.join(os.path.dirname(video_file), "final_video_part2.mp4")
                        output_video_trimmed = os.path.join(os.path.dirname(video_file), "final_video_trimmed.mp4")
                        temp_concat_file = os.path.join(os.path.dirname(video_file), "concat_list.txt")
                        
                        subprocess.run(['ffmpeg', '-y', '-i', video_file, '-to', trim_start,
                                                '-c', 'copy', output_part1], check=True)
                        subprocess.run(['ffmpeg', '-y', '-i', video_file, '-ss', trim_end,
                                                '-c', 'copy', output_part2], check=True)
                        
                        if os.path.exists(output_part1) and os.path.getsize(output_part1) > 0:
                            with open(temp_concat_file, 'w') as f:
                                f.write(f"file '{output_part1}'\n")
                        if os.path.exists(output_part2) and os.path.getsize(output_part2) > 0:
                            with open(temp_concat_file, 'a') as f:
                                f.write(f"file '{output_part2}'\n")
                        
                        subprocess.run(['ffmpeg', '-y', '-f', 'concat', '-safe', '0', '-i', temp_concat_file,
                                                '-c', 'copy', output_video_trimmed], check=True)
                        
                        if os.path.exists(output_video_trimmed) and os.path.getsize(output_video_trimmed) > 0:
                            os.replace(output_video_trimmed, video_file)
                            st.success(f"è§†é¢‘å‰ªè¾‘å®Œæˆï¼")
                            st.info(f"åˆ é™¤äº†ä» {trim_start} åˆ° {trim_end} çš„ç‰‡æ®µ")
                        else:
                            st.error("å‰ªè¾‘å¤±è´¥")
                    except Exception as e:
                        st.error(f"å‰ªè¾‘å¤±è´¥: {str(e)}")
        else:
            st.info("å‰ªè¾‘æœªå¯ç”¨ï¼Œè·³è¿‡")
    
    trimmed_video = os.path.splitext(mp3_file)[0] + ".mp4"
    
    upload_config_file = os.path.join(TEMP_DIR, "subtitles", "upload_config.pkl")
    loaded_title_desc = None
    loaded_tags_list = None
    
    if os.path.exists(upload_config_file):
        try:
            import pickle
            with open(upload_config_file, 'rb') as f:
                loaded_data = pickle.load(f)
            loaded_title_desc = loaded_data.get('title_desc')
            loaded_tags_list = loaded_data.get('tags')
        except Exception:
            pass
    
    with tab7:
        st.header("7ï¸ğŸ“¤ï¸ ä¸Šä¼ Bç«™")
        
        video_file = st.text_input("è§†é¢‘æ–‡ä»¶è·¯å¾„", value=trimmed_video, key="video_file_path_tab7")
        cover_file_path_input = st.text_input("å°é¢æ–‡ä»¶è·¯å¾„", value=cover_file, key="cover_file_path")
        
        default_title = loaded_title_desc if loaded_title_desc else f"(ä¸­é…)è¯·å…ˆä¸‹è½½å­—å¹•è·å–æ ‡é¢˜"
        title = st.text_input("è§†é¢‘æ ‡é¢˜", value=default_title, help="ç•™ç©ºåˆ™ä½¿ç”¨ç¿»è¯‘åçš„æ ‡é¢˜", key="title")
        
        default_tags = ','.join(loaded_tags_list) if loaded_tags_list else "ç§‘æŠ€"
        tags = st.text_input("è§†é¢‘æ ‡ç­¾", value=default_tags, key="tags_tab7")
        
        if loaded_title_desc:
            st.success("å·²ä»ä¸‹è½½å­—å¹•æ­¥éª¤è·å–æ ‡é¢˜å’Œæ ‡ç­¾")
        else:
            st.warning("æœªæ‰¾åˆ°æ ‡é¢˜å’Œæ ‡ç­¾é…ç½®ï¼Œè¯·å…ˆä¸‹è½½å­—å¹•")
        
        bilibili_enabled = st.checkbox("ä¸Šä¼ åˆ°Bç«™", value=False, key="bilibili_enabled")
        
        if bilibili_enabled and st.button("å¼€å§‹ä¸Šä¼ ", type="primary", key="start_upload_btn"):
            if not os.path.exists(video_file):
                st.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_file}")
            elif not os.path.exists(cover_file_path_input):
                st.error(f"å°é¢æ–‡ä»¶ä¸å­˜åœ¨: {cover_file_path_input}")
            else:
                with st.spinner("æ­£åœ¨ä¸Šä¼ åˆ°Bç«™..."):
                    try:
                        credential = Credential(
                            sessdata=BILI_SESSDATA,
                            bili_jct="bcd4ba0d9ab8a7b95485798ed8097d26"
                        )
                        
                        vu_meta = VideoMeta(
                            tid=130,
                            title=title or "(ä¸­é…)AIå¹»è§‰é€ å‡ºç§‘å­¦å‘ç°ï¼Ÿï¼#aiå¹»è§‰",
                            tags=tags.split(',') if tags else ['ç§‘æŠ€'],
                            desc=title or "(ä¸­é…)AIå¹»è§‰é€ å‡ºç§‘å­¦å‘ç°ï¼Ÿï¼#aiå¹»è§‰",
                            cover=cover_file_path_input,
                            no_reprint=True
                        )
                        
                        async def main_upload():
                            page = VideoUploaderPage(
                                path=video_file,
                                title=title or "(ä¸­é…)AIå¹»è§‰é€ å‡ºç§‘å­¦å‘ç°ï¼Ÿï¼#aiå¹»è§‰",
                                description=title or "(ä¸­é…)AIå¹»è§‰é€ å‡ºç§‘å­¦å‘ç°ï¼Ÿï¼#aiå¹»è§‰",
                            )
                            
                            uploader = video_uploader.VideoUploader([page], vu_meta, credential, line=video_uploader.Lines.QN)
                            
                            @uploader.on("__ALL__")
                            async def ev(data):
                                pass
                            
                            await uploader.start()
                            
                        asyncio.run(main_upload())
                        
                        st.success("ä¸Šä¼ å®Œæˆï¼")
                    except Exception as e:
                        import traceback
                        st.error(f"ä¸Šä¼ å¤±è´¥: {str(e)}")
                        st.markdown("### è°ƒè¯•ä¿¡æ¯")
                        st.text(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                        st.text(f"å®Œæ•´é”™è¯¯: {repr(e)}")
                        st.text(f"Traceback:\n{traceback.format_exc()}")
                        
                        st.markdown("### é…ç½®æ£€æŸ¥")
                        st.text(f"BILI_SESSDATA: {'å·²è®¾ç½®' if BILI_SESSDATA else 'æœªè®¾ç½®'} (é•¿åº¦: {len(BILI_SESSDATA)})")
                        st.text(f"BILI_ACCESS_KEY_ID: {'å·²è®¾ç½®' if BILI_ACCESS_KEY_ID else 'æœªè®¾ç½®'}")
                        st.text(f"BILI_ACCESS_KEY_SECRET: {'å·²è®¾ç½®' if BILI_ACCESS_KEY_SECRET else 'æœªè®¾ç½®'}")
                        st.text(f"è§†é¢‘æ–‡ä»¶: {video_file}")
                        st.text(f"å°é¢æ–‡ä»¶: {cover_file_path_input}")
                        st.text(f"è§†é¢‘æ–‡ä»¶å¤§å°: {os.path.getsize(video_file) / 1024 / 1024:.2f} MB" if os.path.exists(video_file) else "è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
                        st.text(f"å°é¢æ–‡ä»¶å¤§å°: {os.path.getsize(cover_file_path_input) / 1024:.2f} KB" if os.path.exists(cover_file_path_input) else "å°é¢æ–‡ä»¶ä¸å­˜åœ¨")

st.markdown("---")
st.info("ğŸ’¡ æ³¨æ„äº‹é¡¹ï¼š")
st.markdown("""
1. API Keyç­‰æ•æ„Ÿä¿¡æ¯å»ºè®®é€šè¿‡HuggingFace Spacesçš„Secretsç®¡ç†ï¼Œä¸è¦ç›´æ¥åœ¨ä»£ç ä¸­ç¡¬ç¼–ç 
2. å¤„ç†å¤§å‹è§†é¢‘æ—¶ï¼ŒTTSè½¬æ¢å’Œè§†é¢‘å¤„ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
3. Bç«™ä¸Šä¼ åŠŸèƒ½éœ€è¦æœ‰æ•ˆçš„sessdataå’Œaccess_key_id
4. è§†é¢‘å‰ªè¾‘åŠŸèƒ½ä¼šæ°¸ä¹…ä¿®æ”¹è§†é¢‘æ–‡ä»¶ï¼Œè¯·è°¨æ…ä½¿ç”¨
5. å»ºè®®å…ˆåœ¨å°è§†é¢‘ä¸Šæµ‹è¯•æµç¨‹ï¼Œç¡®è®¤æ— è¯¯åå†å¤„ç†å¤§è§†é¢‘
""")
